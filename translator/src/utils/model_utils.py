"""
Utilitaires pour la gestion des mod√®les ML
- V√©rification de l'existence locale des mod√®les
- T√©l√©chargement intelligent des mod√®les
- Gestion du cache HuggingFace
"""

import os
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from huggingface_hub import snapshot_download, HfApi
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

logger = logging.getLogger(__name__)

class ModelManager:
    """Gestionnaire intelligent des mod√®les ML"""
    
    def __init__(self, models_path: str):
        self.models_path = Path(models_path)
        self.models_path.mkdir(parents=True, exist_ok=True)
        self.api = HfApi()
        
    def get_model_local_path(self, model_name: str) -> Path:
        """Retourne le chemin local d'un mod√®le"""
        # Convertir le nom du mod√®le en chemin de r√©pertoire
        model_dir = model_name.replace('/', '_')
        return self.models_path / model_dir
    
    def is_model_downloaded(self, model_name: str) -> bool:
        """V√©rifie si un mod√®le est t√©l√©charg√© localement"""
        # Essayer diff√©rents formats de noms de r√©pertoires
        possible_paths = [
            self.get_model_local_path(model_name),  # facebook_nllb-200-distilled-600M
            self.models_path / model_name.split('/')[-1],  # nllb-200-distilled-600M
            self.models_path / model_name.replace('/', '_'),  # facebook_nllb-200-distilled-600M
        ]
        
        for local_path in possible_paths:
            if self._check_model_files(local_path):
                logger.info(f"‚úÖ Mod√®le {model_name} trouv√© localement: {local_path}")
                return True
        
        logger.debug(f"‚ùå Mod√®le {model_name} non trouv√© dans les chemins: {[str(p) for p in possible_paths]}")
        return False
    
    def _check_model_files(self, local_path: Path) -> bool:
        """V√©rifie si un r√©pertoire contient tous les fichiers n√©cessaires"""
        if not local_path.exists() or not local_path.is_dir():
            return False
        
        # V√©rifier les fichiers essentiels
        required_files = [
            'config.json',
            'pytorch_model.bin',  # ou plusieurs fichiers .safetensors
            'tokenizer.json',
            'tokenizer_config.json'
        ]
        
        # V√©rifier aussi les fichiers .safetensors (format moderne)
        safetensors_files = list(local_path.glob('*.safetensors'))
        
        # Si on a des .safetensors, on n'a pas besoin de pytorch_model.bin
        if safetensors_files:
            required_files.remove('pytorch_model.bin')
        
        for file_name in required_files:
            file_path = local_path / file_name
            if not file_path.exists():
                logger.debug(f"‚ùå Fichier manquant: {file_path}")
                return False
        
        return True
    
    def get_model_info(self, model_name: str) -> Optional[Dict]:
        """R√©cup√®re les informations d'un mod√®le depuis HuggingFace"""
        try:
            model_info = self.api.model_info(model_name)
            
            # Extraire la taille de mani√®re s√©curis√©e
            size = 0
            try:
                if hasattr(model_info, 'safetensors') and model_info.safetensors:
                    size = model_info.safetensors.get('total', 0)
                elif hasattr(model_info, 'pytorch') and model_info.pytorch:
                    size = model_info.pytorch.get('total', 0)
                elif hasattr(model_info, 'model_size'):
                    size = model_info.model_size
            except Exception:
                size = 0
            
            return {
                'id': getattr(model_info, 'id', model_name),
                'downloads': getattr(model_info, 'downloads', 0),
                'likes': getattr(model_info, 'likes', 0),
                'size': size,
                'tags': getattr(model_info, 'tags', []) or []
            }
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Impossible de r√©cup√©rer les infos pour {model_name}: {e}")
            # Retourner des informations par d√©faut
            return {
                'id': model_name,
                'downloads': 0,
                'likes': 0,
                'size': 0,
                'tags': []
            }
    
    def download_model_if_needed(self, model_name: str, force_download: bool = False) -> bool:
        """T√©l√©charge un mod√®le seulement s'il n'existe pas localement"""
        if not force_download and self.is_model_downloaded(model_name):
            logger.info(f"üìÅ Mod√®le {model_name} d√©j√† pr√©sent localement")
            return True
        
        try:
            logger.info(f"üì• T√©l√©chargement du mod√®le {model_name}...")
            
            # Utiliser snapshot_download pour un t√©l√©chargement complet
            local_path = self.get_model_local_path(model_name)
            
            # Cr√©er le r√©pertoire si n√©cessaire
            local_path.mkdir(parents=True, exist_ok=True)
            
            # T√©l√©charger le mod√®le
            snapshot_download(
                repo_id=model_name,
                local_dir=str(local_path),
                local_dir_use_symlinks=False,  # Copier les fichiers
                resume_download=True,  # Reprendre si interrompu
                max_retries=3
            )
            
            logger.info(f"‚úÖ Mod√®le {model_name} t√©l√©charg√© avec succ√®s")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur t√©l√©chargement {model_name}: {e}")
            return False
    
    def load_model_locally(self, model_name: str, quantization_level: str = "float16") -> Tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:
        """Charge un mod√®le depuis le stockage local"""
        local_path = self.get_model_local_path(model_name)
        
        if not self.is_model_downloaded(model_name):
            raise FileNotFoundError(f"Mod√®le {model_name} non trouv√© localement: {local_path}")
        
        logger.info(f"üîÑ Chargement local du mod√®le {model_name}")
        
        # Configuration de base
        base_config = {
            'local_files_only': True,  # Forcer l'utilisation locale
            'trust_remote_code': True,
            'low_cpu_mem_usage': True
        }
        
        # Configuration de quantification
        if quantization_level == "float16":
            base_config['torch_dtype'] = torch.float16
        elif quantization_level == "int8":
            # Charger d'abord en float32 puis quantifier
            base_config['torch_dtype'] = torch.float32
        else:
            base_config['torch_dtype'] = torch.float32
        
        # Charger le tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            str(local_path),
            local_files_only=True,
            trust_remote_code=True
        )
        
        # Charger le mod√®le
        model = AutoModelForSeq2SeqLM.from_pretrained(
            str(local_path),
            **base_config
        )
        
        # Quantification int8 si demand√©e
        if quantization_level == "int8":
            try:
                model = torch.quantization.quantize_dynamic(
                    model, {torch.nn.Linear}, dtype=torch.qint8
                )
                logger.info(f"‚úÖ Mod√®le {model_name} quantifi√© en int8")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Quantification int8 √©chou√©e pour {model_name}: {e}")
        
        logger.info(f"‚úÖ Mod√®le {model_name} charg√© localement")
        return model, tokenizer
    
    def get_models_status(self, model_names: List[str]) -> Dict[str, Dict]:
        """Retourne le statut de plusieurs mod√®les"""
        status = {}
        
        for model_name in model_names:
            is_local = self.is_model_downloaded(model_name)
            info = self.get_model_info(model_name)
            
            status[model_name] = {
                'local': is_local,
                'path': str(self.get_model_local_path(model_name)),
                'info': info,
                'size_mb': info.get('size', 0) / (1024 * 1024) if info else 0
            }
        
        return status
    
    def cleanup_incomplete_downloads(self) -> int:
        """Nettoie les t√©l√©chargements incomplets"""
        cleaned = 0
        
        for model_dir in self.models_path.iterdir():
            if not model_dir.is_dir():
                continue
                
            # V√©rifier si le r√©pertoire contient un t√©l√©chargement complet
            if not self.is_model_downloaded(model_dir.name.replace('_', '/')):
                try:
                    import shutil
                    shutil.rmtree(model_dir)
                    logger.info(f"üßπ Nettoyage r√©pertoire incomplet: {model_dir}")
                    cleaned += 1
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur nettoyage {model_dir}: {e}")
        
        return cleaned

def create_model_manager(models_path: str) -> ModelManager:
    """Factory pour cr√©er un gestionnaire de mod√®les"""
    return ModelManager(models_path)
