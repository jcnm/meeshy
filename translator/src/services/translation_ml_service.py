"""
Service de traduction ML unifi√© - Architecture centralis√©e
Un seul service ML qui charge les mod√®les au d√©marrage et sert tous les canaux
"""

import os
import logging
import time
import asyncio
import re
from typing import Dict, Optional, List, Any, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import threading
from pathlib import Path

# CRITIQUE: Charger les variables d'environnement AVANT tout import
try:
    from dotenv import load_dotenv
    # Charger .env puis .env.local (override)
    env_path = Path(__file__).parent.parent.parent / '.env'
    env_local_path = Path(__file__).parent.parent.parent / '.env.local'
    
    if env_path.exists():
        load_dotenv(env_path)
    
    if env_local_path.exists():
        load_dotenv(env_local_path, override=True)
        print(f"üîß [ML-SERVICE] .env.local charg√© depuis: {env_local_path}")
        print(f"üîß [ML-SERVICE] MODELS_PATH: {os.getenv('MODELS_PATH', 'NOT SET')}")
except ImportError:
    print("‚ö†Ô∏è [ML-SERVICE] python-dotenv non disponible")

# Import des settings
from config.settings import get_settings

# CRITIQUE: D√©finir les variables d'environnement AVANT d'importer transformers
# Transformers lit ces variables au moment de l'import
_settings = get_settings()
os.environ['HF_HOME'] = str(_settings.models_path)
os.environ['TRANSFORMERS_CACHE'] = str(_settings.models_path)
os.environ['HUGGINGFACE_HUB_CACHE'] = str(_settings.models_path)
print(f"üîß [ML-SERVICE] Variables HuggingFace d√©finies: {_settings.models_path}")

# Import du module de segmentation pour pr√©servation de structure
from utils.text_segmentation import TextSegmenter

# Import des mod√®les ML optimis√©s
try:
    import torch
    
    # SOLUTION: D√©sactiver les tensors meta avant d'importer les autres modules
    torch._C._disable_meta = True  # D√©sactiver les tensors meta au niveau PyTorch
    
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
    ML_AVAILABLE = True
    
    # Suppression des warnings de retry Xet
    import warnings
    warnings.filterwarnings("ignore", message=".*Retry attempt.*")
    warnings.filterwarnings("ignore", message=".*reqwest.*")
    warnings.filterwarnings("ignore", message=".*xethub.*")
    warnings.filterwarnings("ignore", message=".*IncompleteMessage.*")
    warnings.filterwarnings("ignore", message=".*SendRequest.*")
    
except ImportError:
    ML_AVAILABLE = False
    print("‚ö†Ô∏è Dependencies ML non disponibles")

logger = logging.getLogger(__name__)

@dataclass
class TranslationResult:
    """R√©sultat d'une traduction unifi√©"""
    translated_text: str
    detected_language: str
    confidence: float
    model_used: str
    from_cache: bool
    processing_time: float
    source_channel: str  # 'zmq', 'rest', 'websocket'

class TranslationMLService:
    """
    Service de traduction ML unifi√© - Singleton
    Charge les mod√®les une seule fois au d√©marrage et sert tous les canaux
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        """Singleton pattern pour garantir une seule instance"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, settings, model_type: str = "all", max_workers: int = 4, quantization_level: str = "float16"):
        if self._initialized:
            return
            
        # Charger les settings
        self.settings = settings
        
        self.model_type = model_type
        # OPTIMISATION CPU MULTICORE: Utiliser 16 workers pour AMD 18 cores
        # Laisser 2 cores pour l'OS et les op√©rations syst√®me
        import os
        cpu_workers = min(max_workers, int(os.getenv('ML_MAX_WORKERS', '16')))
        self.max_workers = cpu_workers
        self.quantization_level = quantization_level
        self.executor = ThreadPoolExecutor(max_workers=cpu_workers)
        
        # Mod√®les ML charg√©s (partag√©s entre tous les canaux)
        self.models = {}
        self.tokenizers = {}
        self.pipelines = {}
        
        # Cache thread-local de tokenizers pour √©viter "Already borrowed"
        self._thread_local_tokenizers = {}
        self._tokenizer_lock = threading.Lock()

        # Segmenteur de texte pour pr√©servation de structure
        self.text_segmenter = TextSegmenter(max_segment_length=100)

        # Configuration des mod√®les depuis les settings et .env
        self.models_path = Path(self.settings.models_path)
        logger.info(f"üîç [ML-SERVICE] models_path configur√©: {self.models_path}")
        logger.info(f"üîç [ML-SERVICE] models_path existe: {self.models_path.exists()}")
        logger.info(f"üîç [ML-SERVICE] HF_HOME env: {os.getenv('HF_HOME', 'NOT SET')}")
        logger.info(f"üîç [ML-SERVICE] TRANSFORMERS_CACHE env: {os.getenv('TRANSFORMERS_CACHE', 'NOT SET')}")
        self.device = os.getenv('DEVICE', 'cpu')
        
        self.model_configs = {
            'basic': {
                'model_name': self.settings.basic_model,
                'local_path': self.models_path / self.settings.basic_model,
                'description': f'{self.settings.basic_model} - Mod√®le rapide',
                'device': self.device,
                'priority': 1  # Charg√© en premier
            },
            'medium': {
                'model_name': self.settings.medium_model,
                'local_path': self.models_path / self.settings.medium_model,
                'description': f'{self.settings.medium_model} - Mod√®le √©quilibr√©',
                'device': self.device,
                'priority': 2
            },
            'premium': {
                'model_name': self.settings.premium_model,
                'local_path': self.models_path / self.settings.premium_model,
                'description': f'{self.settings.premium_model} - Mod√®le haute qualit√©',
                'device': self.device,
                'priority': 3
            }
        }
        
        # Mapping des codes de langues NLLB
        self.lang_codes = {
            'fr': 'fra_Latn',
            'en': 'eng_Latn', 
            'es': 'spa_Latn',
            'de': 'deu_Latn',
            'pt': 'por_Latn',
            'zh': 'zho_Hans',
            'ja': 'jpn_Jpan',
            'ar': 'arb_Arab'
        }
        
        # Mapping des noms de langues pour T5 (noms complets)
        self.language_names = {
            'fr': 'French',
            'en': 'English', 
            'es': 'Spanish',
            'de': 'German',
            'pt': 'Portuguese',
            'zh': 'Chinese',
            'ja': 'Japanese',
            'ar': 'Arabic',
            'it': 'Italian',
            'ru': 'Russian',
            'ko': 'Korean',
            'nl': 'Dutch'
        }
        
        # Stats globales (partag√©es entre tous les canaux)
        self.stats = {
            'translations_count': 0,
            'zmq_translations': 0,
            'rest_translations': 0,
            'websocket_translations': 0,
            'avg_processing_time': 0.0,
            'models_loaded': False,
            'startup_time': None
        }
        self.request_times = []
        
        # √âtat d'initialisation
        self.is_initialized = False
        self.is_loading = False
        self._startup_lock = asyncio.Lock()
        
        self._initialized = True
        self._configure_environment()
        logger.info(f"ü§ñ Service ML Unifi√© cr√©√© (Singleton) avec {max_workers} workers")
    
    def _configure_environment(self):
        """Configure les variables d'environnement bas√©es sur les settings"""
        import os
        
        # OPTIMISATION XET: Configuration pour r√©duire les warnings du nouveau syst√®me
        os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
        os.environ['HF_HUB_DISABLE_IMPLICIT_TOKEN'] = '1'
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        
        # OPTIMISATION R√âSEAU: Configuration pour am√©liorer la connectivit√© Docker
        os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'
        os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = str(self.settings.huggingface_timeout)
        os.environ['HF_HUB_DOWNLOAD_RETRY_DELAY'] = '5'
        os.environ['HF_HUB_DOWNLOAD_MAX_RETRIES'] = str(self.settings.model_download_max_retries)
        
        # SOLUTION: D√©sactiver les tensors meta pour √©viter l'erreur Tensor.item()
        os.environ['PYTORCH_DISABLE_META'] = '1'
        os.environ['PYTORCH_FORCE_CUDA'] = '0'  # Forcer CPU si pas de GPU
        os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = '1'
        
        # Configuration pour √©viter les probl√®mes de proxy/corporate network
        # V√©rifier si le fichier de certificats existe, sinon utiliser le syst√®me par d√©faut
        if os.path.exists('/etc/ssl/certs/ca-certificates.crt'):
            os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
            os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
        elif os.path.exists('/etc/ssl/certs/ca-bundle.crt'):
            os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
            os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
        else:
            # Utiliser le syst√®me par d√©faut
            logger.info("‚ö†Ô∏è Fichier de certificats SSL non trouv√©, utilisation du syst√®me par d√©faut")
        
        # Option pour d√©sactiver temporairement la v√©rification SSL si n√©cessaire
        if os.getenv('HF_HUB_DISABLE_SSL_VERIFICATION', '0') == '1':
            os.environ['REQUESTS_CA_BUNDLE'] = ''
            os.environ['CURL_CA_BUNDLE'] = ''
            logger.info("‚ö†Ô∏è V√©rification SSL d√©sactiv√©e pour Hugging Face (HF_HUB_DISABLE_SSL_VERIFICATION=1)")
    
    async def initialize(self) -> bool:
        """Initialise les mod√®les ML une seule fois au d√©marrage"""
        async with self._startup_lock:
            if self.is_initialized:
                logger.info("‚úÖ Service ML d√©j√† initialis√©")
                return True
                
            if self.is_loading:
                logger.info("‚è≥ Initialisation ML en cours...")
                # Attendre que l'initialisation se termine
                while self.is_loading and not self.is_initialized:
                    await asyncio.sleep(0.5)
                return self.is_initialized
            
            self.is_loading = True
            startup_start = time.time()
            
            if not ML_AVAILABLE:
                logger.error("‚ùå Transformers non disponible. Service ML d√©sactiv√©.")
                self.is_loading = False
                return False
            
            try:
                logger.info("üöÄ Initialisation du Service ML Unifi√©...")
                
                # Configuration optimale des threads PyTorch pour AMD multicore
                if ML_AVAILABLE:
                    torch.set_num_threads(16)  # Utiliser 16 threads pour inference
                    torch.set_num_interop_threads(2)  # 2 threads pour op√©rations inter-op
                    logger.info(f"‚öôÔ∏è PyTorch configur√©: {torch.get_num_threads()} threads intra-op, {torch.get_num_interop_threads()} threads inter-op")
                
                logger.info("üìö Chargement des mod√®les NLLB...")
                
                # Charger les mod√®les par ordre de priorit√©
                models_to_load = sorted(
                    self.model_configs.items(), 
                    key=lambda x: x[1]['priority']
                )
                
                for model_type, config in models_to_load:
                    try:
                        await self._load_model(model_type)
                    except Exception as e:
                        logger.error(f"‚ùå Erreur chargement {model_type}: {e}")
                        # Continuer avec les autres mod√®les
                
                # V√©rifier qu'au moins un mod√®le est charg√©
                if not self.models:
                    logger.error("‚ùå Aucun mod√®le ML charg√©")
                    self.is_loading = False
                    return False
                
                startup_time = time.time() - startup_start
                self.stats['startup_time'] = startup_time
                self.stats['models_loaded'] = True
                self.is_initialized = True
                self.is_loading = False
                
                logger.info(f"‚úÖ Service ML Unifi√© initialis√© en {startup_time:.2f}s")
                logger.info(f"üìä Mod√®les charg√©s: {list(self.models.keys())}")
                logger.info(f"üéØ Pr√™t √† servir tous les canaux: ZMQ, REST, WebSocket")
                
                return True
                
            except Exception as e:
                logger.error(f"‚ùå Erreur critique initialisation ML: {e}")
                self.is_loading = False
                return False
    
    def _get_thread_local_tokenizer(self, model_type: str) -> Optional[AutoTokenizer]:
        """Obtient ou cr√©e un tokenizer pour le thread actuel (√©vite 'Already borrowed')"""
        import threading
        thread_id = threading.current_thread().ident
        cache_key = f"{model_type}_{thread_id}"
        
        # V√©rifier le cache thread-local
        if cache_key in self._thread_local_tokenizers:
            return self._thread_local_tokenizers[cache_key]
        
        # Cr√©er un nouveau tokenizer pour ce thread
        with self._tokenizer_lock:
            # Double-check apr√®s acquisition du lock
            if cache_key in self._thread_local_tokenizers:
                return self._thread_local_tokenizers[cache_key]
            
            try:
                model_name = self.model_configs[model_type]['model_name']
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir=str(self.models_path),
                    use_fast=True
                )
                self._thread_local_tokenizers[cache_key] = tokenizer
                logger.debug(f"‚úÖ Tokenizer thread-local cr√©√©: {cache_key}")
                return tokenizer
            except Exception as e:
                logger.error(f"‚ùå Erreur cr√©ation tokenizer thread-local: {e}")
                return None
    
    async def _load_model(self, model_type: str):
        """Charge un mod√®le sp√©cifique depuis local ou HuggingFace"""
        if model_type in self.models:
            return  # D√©j√† charg√©
        
        config = self.model_configs[model_type]
        model_name = config['model_name']
        local_path = config['local_path']
        device = config['device']
        
        logger.info(f"üì• Chargement {model_type}: {model_name}")
        
        # Charger dans un thread pour √©viter de bloquer
        def load_model():
            try:
                # Tokenizer
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name, 
                    cache_dir=str(self.models_path),
                    use_fast=True,  # Tokenizer rapide
                    model_max_length=512  # Limiter la taille
                )
                
                # Mod√®le avec quantification
                # OPTIMISATION CPU: Utiliser float32 au lieu de float16 sur CPU pour √©viter les erreurs
                # et am√©liorer la compatibilit√©. Sur CPU, float16 n'apporte pas d'acc√©l√©ration.
                dtype = torch.float32 if device == "cpu" else (
                    getattr(torch, self.quantization_level) if hasattr(torch, self.quantization_level) else torch.float32
                )
                
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name,
                    cache_dir=str(self.models_path), 
                    torch_dtype=dtype,
                    low_cpu_mem_usage=True,  # Optimisation m√©moire
                    device_map="auto" if device == "cuda" else None
                )
                
                # OPTIMISATION CPU: Mettre le mod√®le en mode eval pour d√©sactiver dropout
                model.eval()
                
                # CORRECTION: Pas de pipeline partag√© pour √©viter "Already borrowed"
                # On cr√©e les pipelines √† la demande dans _ml_translate
                
                return tokenizer, model
                
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement {model_type}: {e}")
                return None, None
        
        # Charger de mani√®re asynchrone
        loop = asyncio.get_event_loop()
        tokenizer, model = await loop.run_in_executor(self.executor, load_model)
        
        if model and tokenizer:
            self.tokenizers[model_type] = tokenizer
            self.models[model_type] = model
            logger.info(f"‚úÖ Mod√®le {model_type} charg√©: {model_name}")
            if local_path.exists():
                logger.info(f"üìÅ Mod√®le disponible en local: {local_path}")
        else:
            raise Exception(f"√âchec chargement {model_type}")
    
    async def translate(self, text: str, source_language: str = "auto", 
                       target_language: str = "en", model_type: str = "basic",
                       source_channel: str = "unknown") -> Dict[str, Any]:
        """
        Interface unique de traduction pour tous les canaux
        source_channel: 'zmq', 'rest', 'websocket'
        """
        start_time = time.time()
        
        try:
            # Validation
            if not text.strip():
                raise ValueError("Text cannot be empty")
            
            # V√©rifier que le service est initialis√©
            if not self.is_initialized:
                logger.warning("Service ML non initialis√©, utilisation du fallback")
                return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)
            
            # Fallback si mod√®le sp√©cifique pas disponible  
            if model_type not in self.models:
                # Utiliser le premier mod√®le disponible
                available_models = list(self.models.keys())
                if available_models:
                    model_type = available_models[0]
                    logger.info(f"Mod√®le demand√© non disponible, utilisation de: {model_type}")
                else:
                    return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)
            
            # D√©tecter la langue source si n√©cessaire
            detected_lang = source_language if source_language != "auto" else self._detect_language(text)
            
            # Traduire avec le vrai mod√®le ML
            translated_text = await self._ml_translate(text, detected_lang, target_language, model_type)
            
            processing_time = time.time() - start_time
            self._update_stats(processing_time, source_channel)
            
            result = {
                'translated_text': translated_text,
                'detected_language': detected_lang,
                'confidence': 0.95,  # Confiance √©lev√©e pour les vrais mod√®les
                'model_used': f"{model_type}_ml",
                'from_cache': False,
                'processing_time': processing_time,
                'source_channel': source_channel
            }
            
            logger.info(f"‚úÖ [ML-{source_channel.upper()}] '{text[:20]}...' ‚Üí '{translated_text[:20]}...' ({processing_time:.3f}s)")
            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur traduction ML [{source_channel}]: {e}")
            # Fallback en cas d'erreur
            return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

    async def translate_with_structure(self, text: str, source_language: str = "auto",
                                      target_language: str = "en", model_type: str = "basic",
                                      source_channel: str = "unknown") -> Dict[str, Any]:
        """
        Traduction avec pr√©servation de structure (paragraphes, emojis, sauts de ligne)

        Cette m√©thode segmente le texte, traduit chaque segment s√©par√©ment,
        puis r√©assemble en pr√©servant la structure originale

        AM√âLIORATION: S√©lection automatique du mod√®le selon la longueur du texte
        """
        start_time = time.time()

        try:
            # Validation
            if not text.strip():
                raise ValueError("Text cannot be empty")

            # AM√âLIORATION: S√©lection automatique du mod√®le selon la longueur
            # - Textes < 50 chars: basic (rapide)
            # - Textes >= 50 chars: medium (meilleure qualit√©)
            # - Textes >= 200 chars: premium si disponible (qualit√© maximale)
            text_length = len(text)
            original_model_type = model_type

            if text_length >= 200 and 'premium' in self.models:
                model_type = 'premium'
                logger.info(f"[STRUCTURED] Text length {text_length} chars ‚Üí Using PREMIUM model for best quality")
            elif text_length >= 50 and 'medium' in self.models:
                model_type = 'medium'
                logger.info(f"[STRUCTURED] Text length {text_length} chars ‚Üí Using MEDIUM model for better quality")
            elif model_type not in self.models and 'basic' in self.models:
                model_type = 'basic'
                logger.info(f"[STRUCTURED] Requested model not available ‚Üí Using BASIC model")

            if model_type != original_model_type:
                logger.info(f"[STRUCTURED] Model switched: {original_model_type} ‚Üí {model_type}")

            # V√©rifier si le texte est court et sans structure complexe
            if len(text) <= 100 and '\n\n' not in text and not self.text_segmenter.extract_emojis(text)[1]:
                # Texte simple, utiliser la traduction standard
                logger.debug(f"[STRUCTURED] Text is simple, using standard translation")
                return await self.translate(text, source_language, target_language, model_type, source_channel)

            logger.info(f"[STRUCTURED] Starting structured translation: {len(text)} chars")

            # V√©rifier que le service est initialis√©
            if not self.is_initialized:
                logger.warning("Service ML non initialis√©, utilisation du fallback")
                return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

            # Fallback si mod√®le sp√©cifique pas disponible
            if model_type not in self.models:
                available_models = list(self.models.keys())
                if available_models:
                    model_type = available_models[0]
                    logger.info(f"Mod√®le demand√© non disponible, utilisation de: {model_type}")
                else:
                    return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

            # D√©tecter la langue source si n√©cessaire
            detected_lang = source_language if source_language != "auto" else self._detect_language(text)

            # 1. Segmenter le texte (extraction emojis + d√©coupage par paragraphes)
            segments, emojis_map = self.text_segmenter.segment_text(text)
            logger.info(f"[STRUCTURED] Text segmented into {len(segments)} parts with {len(emojis_map)} emojis")

            # XXX: PARALL√âLISATION OPPORTUNIT√â #1 - Traduction de segments ind√©pendants
            # TODO: Les segments sont IND√âPENDANTS les uns des autres apr√®s segmentation
            # TODO: Chaque paragraphe/ligne peut √™tre traduit en PARALL√àLE avec asyncio.gather()
            # TODO: Gains potentiels:
            #       - Pour 10 paragraphes de 100 chars chacun: 10x plus rapide
            #       - Pour 50 lignes de liste: 50x plus rapide (si GPU/CPU disponibles)
            #       - Limiter la concurrence selon les ressources: asyncio.Semaphore(max_concurrent=5)
            # TODO: Impl√©mentation sugg√©r√©e:
            #       async def translate_segment(segment):
            #           if segment['type'] == 'paragraph_break':
            #               return segment
            #           return await self._ml_translate(segment['text'], ...)
            #       
            #       tasks = [translate_segment(seg) for seg in segments]
            #       translated_segments = await asyncio.gather(*tasks)
            # TODO: Consid√©rations:
            #       - Pr√©server l'ordre des segments (gather() le fait automatiquement)
            #       - G√©rer les erreurs individuellement (return_exceptions=True)
            #       - Limiter la charge m√©moire GPU avec Semaphore
            
            # 2. Traduire chaque segment (lignes uniquement, les s√©parateurs et code sont pr√©serv√©s)
            # XXX: ACTUELLEMENT S√âQUENTIEL - voir TODO ci-dessus pour parall√©lisation
            translated_segments = []
            for segment in segments:
                segment_type = segment['type']

                # Pr√©server les s√©parateurs, lignes vides et blocs de code
                if segment_type in ['paragraph_break', 'separator', 'empty_line', 'code']:
                    translated_segments.append(segment)
                    if segment_type == 'code':
                        logger.debug(f"[STRUCTURED] Code block preserved (not translated): {segment['text'][:50]}...")
                    continue

                # Traduire uniquement les lignes de texte normal
                if segment_type == 'line':
                    segment_text = segment['text']
                    if segment_text.strip():
                        try:
                            # D√©tecter les placeholders d'emojis AVANT traduction (nouveau format)
                            placeholders_before = re.findall(r'üîπEMOJI_\d+üîπ', segment_text)

                            # AM√âLIORATION: D√©tecter la position des emojis (d√©but, fin, milieu)
                            emoji_positions = {}
                            for placeholder in placeholders_before:
                                pos = segment_text.find(placeholder)
                                length = len(segment_text)

                                # Calculer si c'est au d√©but, fin, ou milieu
                                # D√©but: dans les 10% premiers caract√®res OU juste apr√®s le premier mot
                                # Fin: dans les 10% derniers caract√®res OU juste avant la ponctuation finale
                                if pos <= max(3, length * 0.1):
                                    emoji_positions[placeholder] = 'start'
                                elif pos >= length - max(3, length * 0.1):
                                    emoji_positions[placeholder] = 'end'
                                else:
                                    # V√©rifier si apr√®s un saut de ligne (d√©but de ligne)
                                    if pos > 0 and segment_text[pos-1] == '\n':
                                        emoji_positions[placeholder] = 'line_start'
                                    # V√©rifier si avant un saut de ligne (fin de ligne)
                                    elif pos + len(placeholder) < length and segment_text[pos + len(placeholder)] == '\n':
                                        emoji_positions[placeholder] = 'line_end'
                                    else:
                                        emoji_positions[placeholder] = ('middle', pos / length)

                            logger.debug(f"[STRUCTURED] Emoji positions mapped: {emoji_positions}")

                            translated = await self._ml_translate(
                                segment_text,
                                detected_lang,
                                target_language,
                                model_type
                            )

                            # V√âRIFICATION CRITIQUE: D√©tecter les placeholders APR√àS traduction
                            placeholders_after = re.findall(r'üîπEMOJI_\d+üîπ', translated)

                            # Comparer les placeholders avant/apr√®s
                            if len(placeholders_before) != len(placeholders_after):
                                logger.error(f"[STRUCTURED] ‚ùå EMOJI PLACEHOLDERS LOST during translation!")
                                logger.error(f"    Before: {placeholders_before}")
                                logger.error(f"    After:  {placeholders_after}")
                                logger.error(f"    Original: '{segment_text}'")
                                logger.error(f"    Translated: '{translated}'")

                                # AM√âLIORATION: R√©injecter les placeholders selon leur position d'origine
                                missing_placeholders = set(placeholders_before) - set(placeholders_after)
                                if missing_placeholders:
                                    logger.warning(f"[STRUCTURED] ‚ö†Ô∏è  Attempting to restore {len(missing_placeholders)} lost placeholders")

                                    for placeholder in missing_placeholders:
                                        position_type = emoji_positions.get(placeholder, 'middle')

                                        if position_type == 'start':
                                            # Emoji √©tait au tout d√©but ‚Üí remettre au d√©but
                                            translated = placeholder + ' ' + translated.lstrip()
                                            logger.info(f"[STRUCTURED] ‚úÖ Restored {placeholder} at START (sentence beginning)")

                                        elif position_type == 'end':
                                            # Emoji √©tait √† la toute fin ‚Üí remettre √† la fin
                                            translated = translated.rstrip() + ' ' + placeholder
                                            logger.info(f"[STRUCTURED] ‚úÖ Restored {placeholder} at END (sentence ending)")

                                        elif position_type == 'line_start':
                                            # Emoji √©tait au d√©but d'une ligne ‚Üí chercher un \n et ins√©rer apr√®s
                                            newline_pos = translated.find('\n')
                                            if newline_pos >= 0:
                                                translated = translated[:newline_pos+1] + placeholder + ' ' + translated[newline_pos+1:]
                                            else:
                                                translated = placeholder + ' ' + translated
                                            logger.info(f"[STRUCTURED] ‚úÖ Restored {placeholder} at LINE_START")

                                        elif position_type == 'line_end':
                                            # Emoji √©tait √† la fin d'une ligne ‚Üí chercher un \n et ins√©rer avant
                                            newline_pos = translated.find('\n')
                                            if newline_pos >= 0:
                                                translated = translated[:newline_pos] + ' ' + placeholder + translated[newline_pos:]
                                            else:
                                                translated = translated + ' ' + placeholder
                                            logger.info(f"[STRUCTURED] ‚úÖ Restored {placeholder} at LINE_END")

                                        else:
                                            # Milieu - utiliser le ratio
                                            _, ratio = position_type if isinstance(position_type, tuple) else ('middle', 0.5)
                                            insert_pos = int(len(translated) * ratio)
                                            # S'assurer d'ins√©rer √† un espace pour √©viter de couper un mot
                                            space_pos = translated.find(' ', insert_pos)
                                            if space_pos > 0 and (space_pos - insert_pos) < 10:
                                                insert_pos = space_pos + 1
                                            translated = translated[:insert_pos] + placeholder + ' ' + translated[insert_pos:]
                                            logger.info(f"[STRUCTURED] ‚úÖ Restored {placeholder} at MIDDLE position {insert_pos}")

                            translated_segments.append({
                                'text': translated,
                                'type': segment['type'],
                                'index': segment['index']
                            })
                            seg_type_label = "PARAGRAPH" if segment_type == 'paragraph' else ("LIST ITEM" if segment_type == 'list_item' else ("LINE" if segment_type == 'line' else "SENTENCE"))
                            logger.debug(f"[STRUCTURED] {seg_type_label} {segment['index']} translated: '{segment_text[:30]}...' ‚Üí '{translated[:30]}...'")
                        except Exception as e:
                            logger.error(f"[STRUCTURED] Error translating {segment_type} {segment['index']}: {e}")
                            # En cas d'erreur, garder le texte original
                            translated_segments.append(segment)
                    else:
                        # Texte vide (ne devrait pas arriver)
                        translated_segments.append(segment)

            # 3. R√©assembler le texte traduit
            final_text = self.text_segmenter.reassemble_text(translated_segments, emojis_map)

            processing_time = time.time() - start_time
            self._update_stats(processing_time, source_channel)

            result = {
                'translated_text': final_text,
                'detected_language': detected_lang,
                'confidence': 0.95,
                'model_used': f"{model_type}_ml_structured",
                'from_cache': False,
                'processing_time': processing_time,
                'source_channel': source_channel,
                'segments_count': len(segments),
                'emojis_count': len(emojis_map)
            }

            logger.info(f"‚úÖ [ML-STRUCTURED-{source_channel.upper()}] {len(text)}‚Üí{len(final_text)} chars, {len(segments)} segments, {len(emojis_map)} emojis ({processing_time:.3f}s)")
            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur traduction structur√©e [{source_channel}]: {e}")
            # Fallback vers traduction standard en cas d'erreur
            return await self.translate(text, source_language, target_language, model_type, source_channel)

    async def _ml_translate(self, text: str, source_lang: str, target_lang: str, model_type: str) -> str:
        """
        Traduction avec le vrai mod√®le ML - tokenizers thread-local pour √©viter 'Already borrowed'
        
        XXX: PARALL√âLISATION OPPORTUNIT√â #2 - Traduction batch pour multiples segments
        TODO: Cette m√©thode pourrait accepter une LISTE de textes au lieu d'un seul
        TODO: Avantages du batch processing:
              - R√©duire l'overhead de cr√©ation de pipeline (1 fois au lieu de N fois)
              - Utiliser batch_size optimal du mod√®le (traiter 8-16 segments √† la fois)
              - Meilleure utilisation GPU/CPU (pas de temps mort entre segments)
        TODO: Signature sugg√©r√©e:
              async def _ml_translate_batch(
                  self, 
                  texts: List[str], 
                  source_lang: str, 
                  target_lang: str, 
                  model_type: str
              ) -> List[str]:
                  # Cr√©er pipeline UNE fois
                  # Traduire tous les textes en batch_size chunks
                  # Retourner r√©sultats dans le m√™me ordre
        TODO: Gains attendus:
              - 3-5x plus rapide pour 10+ segments
              - R√©duction de 70% du temps de setup (pipeline creation)
              - Meilleure utilisation m√©moire GPU
        """
        try:
            if model_type not in self.models:
                raise Exception(f"Mod√®le {model_type} non charg√©")
            
            # CORRECTION: Sauvegarder le model_name original pour √©viter les collisions dans la boucle de fallback
            original_model_name = self.model_configs[model_type]['model_name']
            
            # Traduction dans un thread - OPTIMISATION: tokenizer thread-local cach√©
            def translate():
                try:
                    from transformers import pipeline
                    import threading
                    
                    # Mod√®le partag√© (thread-safe en lecture)
                    shared_model = self.models[model_type]
                    
                    # OPTIMISATION: Utiliser le tokenizer thread-local cach√© (√©vite recr√©ation)
                    thread_tokenizer = self._get_thread_local_tokenizer(model_type)
                    if thread_tokenizer is None:
                        raise Exception(f"Impossible d'obtenir le tokenizer pour {model_type}")
                    
                    # Diff√©rencier T5 et NLLB avec pipelines appropri√©s
                    if "t5" in original_model_name.lower():
                        # T5: utiliser text2text-generation avec tokenizer thread-local
                        # OPTIMISATION CPU: R√©duire max_length pour acc√©l√©rer les inf√©rences
                        temp_pipeline = pipeline(
                            "text2text-generation",
                            model=shared_model,
                            tokenizer=thread_tokenizer,  # ‚Üê TOKENIZER THREAD-LOCAL
                            device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                            max_length=256,  # Augment√© pour traductions de qualit√©
                            batch_size=8  # Optimis√© pour multicore AMD
                        )
                        
                        # T5: format avec noms complets de langues
                        source_name = self.language_names.get(source_lang, source_lang.capitalize())
                        target_name = self.language_names.get(target_lang, target_lang.capitalize())
                        instruction = f"translate {source_name} to {target_name}: {text}"
                        
                        logger.debug(f"T5 instruction: {instruction}")
                        
                        # OPTIMISATION MULTICORE: Param√®tres optimis√©s pour AMD 18 cores
                        # num_beams=4 pour qualit√© avec multicore
                        result = temp_pipeline(
                            instruction,
                            max_new_tokens=256,  # Augment√© pour traductions compl√®tes
                            num_beams=4,  # √âquilibre qualit√©/vitesse
                            do_sample=False,
                            early_stopping=True,
                            repetition_penalty=1.1,
                            length_penalty=1.0
                        )
                        
                        # T5 retourne generated_text
                        t5_success = False
                        translated = None  # CORRECTION: Initialiser translated pour √©viter UnboundLocalError
                        if result and len(result) > 0 and 'generated_text' in result[0]:
                            raw_text = result[0]['generated_text']
                            
                            # Nettoyer l'instruction si pr√©sente dans le r√©sultat
                            instruction_prefix = f"translate {source_name} to {target_name}:"
                            if instruction_prefix in raw_text:
                                parts = raw_text.split(instruction_prefix, 1)
                                if len(parts) > 1:
                                    translated = parts[1].strip()
                                else:
                                    translated = raw_text.strip()
                            else:
                                translated = raw_text.strip()
                                
                            # Validation: v√©rifier si T5 a vraiment traduit (pas juste r√©p√©t√© l'instruction)
                            # Rejeter SEULEMENT si:
                            # 1. Vide
                            # 2. Identique √† l'original
                            # 3. Contient l'instruction compl√®te T5 (ex: "translate French to Spanish:")
                            has_instruction = f"translate {source_name} to {target_name}:" in translated.lower()
                            
                            if not translated or translated.lower() == text.lower() or has_instruction:
                                logger.warning(f"T5 traduction invalide: '{translated}', fallback vers NLLB")
                                t5_success = False
                            else:
                                t5_success = True
                                
                        # Si T5 √©choue, fallback automatique vers NLLB
                        if not t5_success:
                            logger.info(f"Fallback : T5 ‚Üí NLLB {source_lang}‚Üí{target_lang}")
                            # Nettoyer le pipeline T5 (tokenizer reste en cache)
                            del temp_pipeline
                            
                            # CORRECTION: Chercher un mod√®le NLLB parmi les mod√®les charg√©s
                            # Les cl√©s sont 'basic', 'medium', 'premium', pas les noms de mod√®les
                            nllb_model_type = None
                            nllb_model_name = None
                            
                            # Chercher medium ou premium (qui sont des mod√®les NLLB)
                            # CORRECTION: Utiliser un nom de variable diff√©rent pour √©viter la collision
                            for fallback_model_type in ['medium', 'premium']:
                                if fallback_model_type in self.models:
                                    config = self.model_configs.get(fallback_model_type, {})
                                    fallback_model_name = config.get('model_name', '')
                                    if 'nllb' in fallback_model_name.lower():
                                        nllb_model_type = fallback_model_type
                                        nllb_model_name = fallback_model_name
                                        break
                            
                            if nllb_model_type is None:
                                logger.warning(f"Mod√®le NLLB non charg√©, impossible de faire le fallback")
                                translated = f"[Translation-Failed] {text}"
                            else:
                                nllb_model = self.models[nllb_model_type]
                                # OPTIMISATION: Utiliser tokenizer thread-local cach√©
                                nllb_tokenizer = self._get_thread_local_tokenizer(nllb_model_type)
                                if nllb_tokenizer is None:
                                    logger.warning(f"Impossible d'obtenir tokenizer NLLB")
                                    translated = f"[Translation-Failed] {text}"
                                    return translated
                                
                                # OPTIMISATION MULTICORE: Param√®tres optimis√©s pour AMD 18 cores
                                nllb_pipeline = pipeline(
                                    "translation",
                                    model=nllb_model,
                                    tokenizer=nllb_tokenizer,
                                    device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                                    max_length=512,  # Augment√© pour qualit√©
                                    batch_size=8  # Optimis√© pour multicore
                                )
                                
                                nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                                nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')
                                
                                nllb_result = nllb_pipeline(
                                    text, 
                                    src_lang=nllb_source, 
                                    tgt_lang=nllb_target, 
                                    max_length=512,  # Augment√© pour qualit√©
                                    num_beams=4,  # √âquilibre qualit√©/vitesse
                                    early_stopping=True
                                )
                                
                                if nllb_result and len(nllb_result) > 0 and 'translation_text' in nllb_result[0]:
                                    translated = nllb_result[0]['translation_text']
                                    logger.info(f"‚úÖ Fallback NLLB r√©ussi: '{text}' ‚Üí '{translated}'")
                                else:
                                    translated = f"[NLLB-Fallback-Failed] {text}"
                                
                                # Nettoyer le pipeline (tokenizer reste en cache)
                                del nllb_pipeline
                            
                            return translated
                            
                    else:
                        # NLLB: utiliser translation avec tokenizer thread-local
                        # OPTIMISATION MULTICORE: Param√®tres optimis√©s pour AMD 18 cores
                        temp_pipeline = pipeline(
                            "translation",
                            model=shared_model,
                            tokenizer=thread_tokenizer,  # ‚Üê TOKENIZER THREAD-LOCAL
                            device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                            max_length=512,  # Augment√© pour qualit√©
                            batch_size=8  # Optimis√© pour multicore
                        )
                        
                        # NLLB: codes de langue sp√©ciaux
                        nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                        nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')
                        
                        # OPTIMISATION MULTICORE: Param√®tres optimis√©s pour qualit√© et vitesse
                        result = temp_pipeline(
                            text, 
                            src_lang=nllb_source, 
                            tgt_lang=nllb_target, 
                            max_length=512,  # Augment√© pour qualit√©
                            num_beams=4,  # √âquilibre qualit√©/vitesse sur multicore
                            early_stopping=True
                        )
                        
                        # NLLB retourne translation_text
                        if result and len(result) > 0 and 'translation_text' in result[0]:
                            translated = result[0]['translation_text']
                        else:
                            translated = f"[NLLB-No-Result] {text}"
                    
                    # Nettoyer tokenizer et pipeline temporaires
                    del thread_tokenizer
                    del temp_pipeline
                    
                    return translated
                    
                except Exception as e:
                    logger.error(f"Erreur pipeline {original_model_name}: {e}")
                    return f"[ML-Pipeline-Error] {text}"
            
            # Ex√©cuter de mani√®re asynchrone
            loop = asyncio.get_event_loop()
            translated = await loop.run_in_executor(self.executor, translate)
            
            return translated
            
        except Exception as e:
            logger.error(f"‚ùå Erreur mod√®le ML {model_type}: {e}")
            return f"[ML-Error] {text}"
    
    def _detect_language(self, text: str) -> str:
        """D√©tection de langue simple"""
        text_lower = text.lower()
        
        # Mots caract√©ristiques par langue
        if any(word in text_lower for word in ['bonjour', 'comment', 'vous', 'merci', 'salut']):
            return 'fr'
        elif any(word in text_lower for word in ['hello', 'how', 'you', 'thank', 'hi']):
            return 'en'
        elif any(word in text_lower for word in ['hola', 'como', 'estas', 'gracias']):
            return 'es'
        elif any(word in text_lower for word in ['guten', 'wie', 'geht', 'danke', 'hallo']):
            return 'de'
        else:
            return 'en'  # D√©faut
    
    async def _fallback_translate(self, text: str, source_lang: str, target_lang: str, 
                                 model_type: str, source_channel: str) -> Dict[str, Any]:
        """Traduction de fallback si ML non disponible"""
        logger.warning(f"Utilisation du fallback pour {model_type} [{source_channel}]")
        
        # Dictionnaire simple comme fallback
        translations = {
            ('fr', 'en'): {
                'bonjour': 'hello', 'comment': 'how', 'vous': 'you', 'allez': 'are',
                '√™tes': 'are', 'tout': 'all', 'le': 'the', 'monde': 'world'
            },
            ('en', 'fr'): {
                'hello': 'bonjour', 'how': 'comment', 'you': 'vous', 'are': '√™tes',
                'all': 'tout', 'the': 'le', 'world': 'monde'
            },
            ('es', 'fr'): {
                'hola': 'bonjour', 'como': 'comment', 'estas': 'allez-vous'
            },
            ('en', 'de'): {
                'hello': 'hallo', 'how': 'wie', 'are': 'sind', 'you': 'sie'
            }
        }
        
        # Traduction simple mot par mot
        lang_pair = (source_lang, target_lang)
        if lang_pair in translations:
            words = text.lower().split()
            translated_words = []
            for word in words:
                translated_word = translations[lang_pair].get(word, word)
                translated_words.append(translated_word)
            translated_text = ' '.join(translated_words)
        else:
            translated_text = f"[FALLBACK-{source_lang}‚Üí{target_lang}] {text}"
        
        self._update_stats(0.001, source_channel)
        
        return {
            'translated_text': translated_text,
            'detected_language': source_lang,
            'confidence': 0.3,  # Faible confiance pour fallback
            'model_used': f"{model_type}_fallback",
            'from_cache': False,
            'processing_time': 0.001,
            'source_channel': source_channel
        }
    
    def _update_stats(self, processing_time: float, source_channel: str):
        """Met √† jour les statistiques globales"""
        self.stats['translations_count'] += 1
        
        # Mettre √† jour les stats par canal (canaux connus seulement)
        if source_channel in ['zmq', 'rest', 'websocket']:
            self.stats[f'{source_channel}_translations'] += 1
        
        self.request_times.append(processing_time)
        
        if len(self.request_times) > 200:
            self.request_times = self.request_times[-200:]
        
        if self.request_times:
            self.stats['avg_processing_time'] = sum(self.request_times) / len(self.request_times)
    
    async def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques globales du service"""
        return {
            'service_type': 'unified_ml',
            'is_singleton': True,
            'translations_count': self.stats['translations_count'],
            'zmq_translations': self.stats['zmq_translations'],
            'rest_translations': self.stats['rest_translations'], 
            'websocket_translations': self.stats['websocket_translations'],
            'avg_processing_time': self.stats['avg_processing_time'],
            'models_loaded': {
                model_type: {
                    'name': self.model_configs[model_type]['model_name'],
                    'description': self.model_configs[model_type]['description'],
                    'local_path': str(self.model_configs[model_type]['local_path']),
                    'is_local': self.model_configs[model_type]['local_path'].exists()
                } for model_type in self.models.keys()
            },
            'ml_available': ML_AVAILABLE,
            'is_initialized': self.is_initialized,
            'startup_time': self.stats['startup_time'],
            'supported_languages': list(self.lang_codes.keys()),
            'models_path': str(self.models_path),
            'device': self.device
        }
    
    async def get_health(self) -> Dict[str, Any]:
        """Health check du service unifi√©"""
        return {
            'status': 'healthy' if self.is_initialized else 'initializing',
            'models_count': len(self.models),
            'pipelines_count': len(self.pipelines),
            'ml_available': ML_AVAILABLE,
            'translations_served': self.stats['translations_count']
        }

# Instance globale du service (Singleton)
def get_unified_ml_service(max_workers: int = 4) -> TranslationMLService:
    """Retourne l'instance unique du service ML"""
    return TranslationMLService(get_settings(), max_workers=max_workers)
