"""
Service de traduction ML unifi√© - Architecture centralis√©e
Un seul service ML qui charge les mod√®les au d√©marrage et sert tous les canaux
"""

import os
import logging
import time
import asyncio
from typing import Dict, Optional, List, Any, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import threading
from pathlib import Path

# Import des settings
from config.settings import get_settings

# Import du module de segmentation pour pr√©servation de structure
from utils.text_segmentation import TextSegmenter

# Import des mod√®les ML optimis√©s
try:
    import torch
    
    # SOLUTION: D√©sactiver les tensors meta avant d'importer les autres modules
    torch._C._disable_meta = True  # D√©sactiver les tensors meta au niveau PyTorch
    
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
    ML_AVAILABLE = True
    
    # Suppression des warnings de retry Xet
    import warnings
    warnings.filterwarnings("ignore", message=".*Retry attempt.*")
    warnings.filterwarnings("ignore", message=".*reqwest.*")
    warnings.filterwarnings("ignore", message=".*xethub.*")
    warnings.filterwarnings("ignore", message=".*IncompleteMessage.*")
    warnings.filterwarnings("ignore", message=".*SendRequest.*")
    
except ImportError:
    ML_AVAILABLE = False
    print("‚ö†Ô∏è Dependencies ML non disponibles")

logger = logging.getLogger(__name__)

@dataclass
class TranslationResult:
    """R√©sultat d'une traduction unifi√©"""
    translated_text: str
    detected_language: str
    confidence: float
    model_used: str
    from_cache: bool
    processing_time: float
    source_channel: str  # 'zmq', 'rest', 'websocket'

class TranslationMLService:
    """
    Service de traduction ML unifi√© - Singleton
    Charge les mod√®les une seule fois au d√©marrage et sert tous les canaux
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        """Singleton pattern pour garantir une seule instance"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, settings, model_type: str = "all", max_workers: int = 4, quantization_level: str = "float16"):
        if self._initialized:
            return
            
        # Charger les settings
        self.settings = settings
        
        self.model_type = model_type
        # OPTIMISATION CPU: Limiter les workers pour √©viter le context switching
        # Sur CPU, 2-4 workers suffisent largement
        import os
        cpu_workers = min(max_workers, int(os.getenv('ML_MAX_WORKERS', '4')))
        self.max_workers = cpu_workers
        self.quantization_level = quantization_level
        self.executor = ThreadPoolExecutor(max_workers=cpu_workers)
        
        # Mod√®les ML charg√©s (partag√©s entre tous les canaux)
        self.models = {}
        self.tokenizers = {}
        self.pipelines = {}
        
        # Cache thread-local de tokenizers pour √©viter "Already borrowed"
        self._thread_local_tokenizers = {}
        self._tokenizer_lock = threading.Lock()

        # Segmenteur de texte pour pr√©servation de structure
        self.text_segmenter = TextSegmenter(max_segment_length=100)

        # Configuration des mod√®les depuis les settings et .env
        self.models_path = Path(self.settings.models_path)
        self.device = os.getenv('DEVICE', 'cpu')
        
        self.model_configs = {
            'basic': {
                'model_name': self.settings.basic_model,
                'local_path': self.models_path / self.settings.basic_model,
                'description': f'{self.settings.basic_model} - Mod√®le rapide',
                'device': self.device,
                'priority': 1  # Charg√© en premier
            },
            'medium': {
                'model_name': self.settings.medium_model,
                'local_path': self.models_path / self.settings.medium_model,
                'description': f'{self.settings.medium_model} - Mod√®le √©quilibr√©',
                'device': self.device,
                'priority': 2
            },
            'premium': {
                'model_name': self.settings.premium_model,
                'local_path': self.models_path / self.settings.premium_model,
                'description': f'{self.settings.premium_model} - Mod√®le haute qualit√©',
                'device': self.device,
                'priority': 3
            }
        }
        
        # Mapping des codes de langues NLLB
        self.lang_codes = {
            'fr': 'fra_Latn',
            'en': 'eng_Latn', 
            'es': 'spa_Latn',
            'de': 'deu_Latn',
            'pt': 'por_Latn',
            'zh': 'zho_Hans',
            'ja': 'jpn_Jpan',
            'ar': 'arb_Arab'
        }
        
        # Mapping des noms de langues pour T5 (noms complets)
        self.language_names = {
            'fr': 'French',
            'en': 'English', 
            'es': 'Spanish',
            'de': 'German',
            'pt': 'Portuguese',
            'zh': 'Chinese',
            'ja': 'Japanese',
            'ar': 'Arabic',
            'it': 'Italian',
            'ru': 'Russian',
            'ko': 'Korean',
            'nl': 'Dutch'
        }
        
        # Stats globales (partag√©es entre tous les canaux)
        self.stats = {
            'translations_count': 0,
            'zmq_translations': 0,
            'rest_translations': 0,
            'websocket_translations': 0,
            'avg_processing_time': 0.0,
            'models_loaded': False,
            'startup_time': None
        }
        self.request_times = []
        
        # √âtat d'initialisation
        self.is_initialized = False
        self.is_loading = False
        self._startup_lock = asyncio.Lock()
        
        self._initialized = True
        self._configure_environment()
        logger.info(f"ü§ñ Service ML Unifi√© cr√©√© (Singleton) avec {max_workers} workers")
    
    def _configure_environment(self):
        """Configure les variables d'environnement bas√©es sur les settings"""
        import os
        
        # OPTIMISATION XET: Configuration pour r√©duire les warnings du nouveau syst√®me
        os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
        os.environ['HF_HUB_DISABLE_IMPLICIT_TOKEN'] = '1'
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        
        # OPTIMISATION R√âSEAU: Configuration pour am√©liorer la connectivit√© Docker
        os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'
        os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = str(self.settings.huggingface_timeout)
        os.environ['HF_HUB_DOWNLOAD_RETRY_DELAY'] = '5'
        os.environ['HF_HUB_DOWNLOAD_MAX_RETRIES'] = str(self.settings.model_download_max_retries)
        
        # SOLUTION: D√©sactiver les tensors meta pour √©viter l'erreur Tensor.item()
        os.environ['PYTORCH_DISABLE_META'] = '1'
        os.environ['PYTORCH_FORCE_CUDA'] = '0'  # Forcer CPU si pas de GPU
        os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = '1'
        
        # Configuration pour √©viter les probl√®mes de proxy/corporate network
        # V√©rifier si le fichier de certificats existe, sinon utiliser le syst√®me par d√©faut
        if os.path.exists('/etc/ssl/certs/ca-certificates.crt'):
            os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
            os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
        elif os.path.exists('/etc/ssl/certs/ca-bundle.crt'):
            os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
            os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
        else:
            # Utiliser le syst√®me par d√©faut
            logger.info("‚ö†Ô∏è Fichier de certificats SSL non trouv√©, utilisation du syst√®me par d√©faut")
        
        # Option pour d√©sactiver temporairement la v√©rification SSL si n√©cessaire
        if os.getenv('HF_HUB_DISABLE_SSL_VERIFICATION', '0') == '1':
            os.environ['REQUESTS_CA_BUNDLE'] = ''
            os.environ['CURL_CA_BUNDLE'] = ''
            logger.info("‚ö†Ô∏è V√©rification SSL d√©sactiv√©e pour Hugging Face (HF_HUB_DISABLE_SSL_VERIFICATION=1)")
    
    async def initialize(self) -> bool:
        """Initialise les mod√®les ML une seule fois au d√©marrage"""
        async with self._startup_lock:
            if self.is_initialized:
                logger.info("‚úÖ Service ML d√©j√† initialis√©")
                return True
                
            if self.is_loading:
                logger.info("‚è≥ Initialisation ML en cours...")
                # Attendre que l'initialisation se termine
                while self.is_loading and not self.is_initialized:
                    await asyncio.sleep(0.5)
                return self.is_initialized
            
            self.is_loading = True
            startup_start = time.time()
            
            if not ML_AVAILABLE:
                logger.error("‚ùå Transformers non disponible. Service ML d√©sactiv√©.")
                self.is_loading = False
                return False
            
            try:
                logger.info("üöÄ Initialisation du Service ML Unifi√©...")
                logger.info("üìö Chargement des mod√®les NLLB...")
                
                # Charger les mod√®les par ordre de priorit√©
                models_to_load = sorted(
                    self.model_configs.items(), 
                    key=lambda x: x[1]['priority']
                )
                
                for model_type, config in models_to_load:
                    try:
                        await self._load_model(model_type)
                    except Exception as e:
                        logger.error(f"‚ùå Erreur chargement {model_type}: {e}")
                        # Continuer avec les autres mod√®les
                
                # V√©rifier qu'au moins un mod√®le est charg√©
                if not self.models:
                    logger.error("‚ùå Aucun mod√®le ML charg√©")
                    self.is_loading = False
                    return False
                
                startup_time = time.time() - startup_start
                self.stats['startup_time'] = startup_time
                self.stats['models_loaded'] = True
                self.is_initialized = True
                self.is_loading = False
                
                logger.info(f"‚úÖ Service ML Unifi√© initialis√© en {startup_time:.2f}s")
                logger.info(f"üìä Mod√®les charg√©s: {list(self.models.keys())}")
                logger.info(f"üéØ Pr√™t √† servir tous les canaux: ZMQ, REST, WebSocket")
                
                return True
                
            except Exception as e:
                logger.error(f"‚ùå Erreur critique initialisation ML: {e}")
                self.is_loading = False
                return False
    
    def _get_thread_local_tokenizer(self, model_type: str) -> Optional[AutoTokenizer]:
        """Obtient ou cr√©e un tokenizer pour le thread actuel (√©vite 'Already borrowed')"""
        import threading
        thread_id = threading.current_thread().ident
        cache_key = f"{model_type}_{thread_id}"
        
        # V√©rifier le cache thread-local
        if cache_key in self._thread_local_tokenizers:
            return self._thread_local_tokenizers[cache_key]
        
        # Cr√©er un nouveau tokenizer pour ce thread
        with self._tokenizer_lock:
            # Double-check apr√®s acquisition du lock
            if cache_key in self._thread_local_tokenizers:
                return self._thread_local_tokenizers[cache_key]
            
            try:
                model_name = self.model_configs[model_type]['model_name']
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir=str(self.models_path),
                    use_fast=True
                )
                self._thread_local_tokenizers[cache_key] = tokenizer
                logger.debug(f"‚úÖ Tokenizer thread-local cr√©√©: {cache_key}")
                return tokenizer
            except Exception as e:
                logger.error(f"‚ùå Erreur cr√©ation tokenizer thread-local: {e}")
                return None
    
    async def _load_model(self, model_type: str):
        """Charge un mod√®le sp√©cifique depuis local ou HuggingFace"""
        if model_type in self.models:
            return  # D√©j√† charg√©
        
        config = self.model_configs[model_type]
        model_name = config['model_name']
        local_path = config['local_path']
        device = config['device']
        
        logger.info(f"üì• Chargement {model_type}: {model_name}")
        
        # Charger dans un thread pour √©viter de bloquer
        def load_model():
            try:
                # Tokenizer
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name, 
                    cache_dir=str(self.models_path),
                    use_fast=True,  # Tokenizer rapide
                    model_max_length=512  # Limiter la taille
                )
                
                # Mod√®le avec quantification
                # OPTIMISATION CPU: Utiliser float32 au lieu de float16 sur CPU pour √©viter les erreurs
                # et am√©liorer la compatibilit√©. Sur CPU, float16 n'apporte pas d'acc√©l√©ration.
                dtype = torch.float32 if device == "cpu" else (
                    getattr(torch, self.quantization_level) if hasattr(torch, self.quantization_level) else torch.float32
                )
                
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name,
                    cache_dir=str(self.models_path), 
                    torch_dtype=dtype,
                    low_cpu_mem_usage=True,  # Optimisation m√©moire
                    device_map="auto" if device == "cuda" else None
                )
                
                # OPTIMISATION CPU: Mettre le mod√®le en mode eval pour d√©sactiver dropout
                model.eval()
                
                # CORRECTION: Pas de pipeline partag√© pour √©viter "Already borrowed"
                # On cr√©e les pipelines √† la demande dans _ml_translate
                
                return tokenizer, model
                
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement {model_type}: {e}")
                return None, None
        
        # Charger de mani√®re asynchrone
        loop = asyncio.get_event_loop()
        tokenizer, model = await loop.run_in_executor(self.executor, load_model)
        
        if model and tokenizer:
            self.tokenizers[model_type] = tokenizer
            self.models[model_type] = model
            logger.info(f"‚úÖ Mod√®le {model_type} charg√©: {model_name}")
            if local_path.exists():
                logger.info(f"üìÅ Mod√®le disponible en local: {local_path}")
        else:
            raise Exception(f"√âchec chargement {model_type}")
    
    async def translate(self, text: str, source_language: str = "auto", 
                       target_language: str = "en", model_type: str = "basic",
                       source_channel: str = "unknown") -> Dict[str, Any]:
        """
        Interface unique de traduction pour tous les canaux
        source_channel: 'zmq', 'rest', 'websocket'
        """
        start_time = time.time()
        
        try:
            # Validation
            if not text.strip():
                raise ValueError("Text cannot be empty")
            
            # V√©rifier que le service est initialis√©
            if not self.is_initialized:
                logger.warning("Service ML non initialis√©, utilisation du fallback")
                return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)
            
            # Fallback si mod√®le sp√©cifique pas disponible  
            if model_type not in self.models:
                # Utiliser le premier mod√®le disponible
                available_models = list(self.models.keys())
                if available_models:
                    model_type = available_models[0]
                    logger.info(f"Mod√®le demand√© non disponible, utilisation de: {model_type}")
                else:
                    return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)
            
            # D√©tecter la langue source si n√©cessaire
            detected_lang = source_language if source_language != "auto" else self._detect_language(text)
            
            # Traduire avec le vrai mod√®le ML
            translated_text = await self._ml_translate(text, detected_lang, target_language, model_type)
            
            processing_time = time.time() - start_time
            self._update_stats(processing_time, source_channel)
            
            result = {
                'translated_text': translated_text,
                'detected_language': detected_lang,
                'confidence': 0.95,  # Confiance √©lev√©e pour les vrais mod√®les
                'model_used': f"{model_type}_ml",
                'from_cache': False,
                'processing_time': processing_time,
                'source_channel': source_channel
            }
            
            logger.info(f"‚úÖ [ML-{source_channel.upper()}] '{text[:20]}...' ‚Üí '{translated_text[:20]}...' ({processing_time:.3f}s)")
            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur traduction ML [{source_channel}]: {e}")
            # Fallback en cas d'erreur
            return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

    async def translate_with_structure(self, text: str, source_language: str = "auto",
                                      target_language: str = "en", model_type: str = "basic",
                                      source_channel: str = "unknown") -> Dict[str, Any]:
        """
        Traduction avec pr√©servation de structure (paragraphes, emojis, sauts de ligne)

        Cette m√©thode segmente le texte, traduit chaque segment s√©par√©ment,
        puis r√©assemble en pr√©servant la structure originale
        """
        start_time = time.time()

        try:
            # Validation
            if not text.strip():
                raise ValueError("Text cannot be empty")

            # V√©rifier si le texte est court et sans structure complexe
            if len(text) <= 100 and '\n\n' not in text and not self.text_segmenter.extract_emojis(text)[1]:
                # Texte simple, utiliser la traduction standard
                logger.debug(f"[STRUCTURED] Text is simple, using standard translation")
                return await self.translate(text, source_language, target_language, model_type, source_channel)

            logger.info(f"[STRUCTURED] Starting structured translation: {len(text)} chars")

            # V√©rifier que le service est initialis√©
            if not self.is_initialized:
                logger.warning("Service ML non initialis√©, utilisation du fallback")
                return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

            # Fallback si mod√®le sp√©cifique pas disponible
            if model_type not in self.models:
                available_models = list(self.models.keys())
                if available_models:
                    model_type = available_models[0]
                    logger.info(f"Mod√®le demand√© non disponible, utilisation de: {model_type}")
                else:
                    return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)

            # D√©tecter la langue source si n√©cessaire
            detected_lang = source_language if source_language != "auto" else self._detect_language(text)

            # 1. Segmenter le texte (extraction emojis + d√©coupage par paragraphes)
            segments, emojis_map = self.text_segmenter.segment_text(text)
            logger.info(f"[STRUCTURED] Text segmented into {len(segments)} parts with {len(emojis_map)} emojis")

            # 2. Traduire chaque segment
            translated_segments = []
            for segment in segments:
                if segment['type'] == 'empty_line':
                    # Pr√©server les lignes vides
                    translated_segments.append(segment)
                else:
                    # Traduire le segment
                    segment_text = segment['text']
                    if segment_text.strip():
                        try:
                            translated = await self._ml_translate(
                                segment_text,
                                detected_lang,
                                target_lang,
                                model_type
                            )
                            translated_segments.append({
                                'text': translated,
                                'type': segment['type'],
                                'index': segment['index']
                            })
                            logger.debug(f"[STRUCTURED] Segment {segment['index']} translated: '{segment_text[:30]}...' ‚Üí '{translated[:30]}...'")
                        except Exception as e:
                            logger.error(f"[STRUCTURED] Error translating segment {segment['index']}: {e}")
                            # En cas d'erreur, garder le texte original
                            translated_segments.append(segment)
                    else:
                        translated_segments.append(segment)

            # 3. R√©assembler le texte traduit
            final_text = self.text_segmenter.reassemble_text(translated_segments, emojis_map)

            processing_time = time.time() - start_time
            self._update_stats(processing_time, source_channel)

            result = {
                'translated_text': final_text,
                'detected_language': detected_lang,
                'confidence': 0.95,
                'model_used': f"{model_type}_ml_structured",
                'from_cache': False,
                'processing_time': processing_time,
                'source_channel': source_channel,
                'segments_count': len(segments),
                'emojis_count': len(emojis_map)
            }

            logger.info(f"‚úÖ [ML-STRUCTURED-{source_channel.upper()}] {len(text)}‚Üí{len(final_text)} chars, {len(segments)} segments, {len(emojis_map)} emojis ({processing_time:.3f}s)")
            return result

        except Exception as e:
            logger.error(f"‚ùå Erreur traduction structur√©e [{source_channel}]: {e}")
            # Fallback vers traduction standard en cas d'erreur
            return await self.translate(text, source_language, target_language, model_type, source_channel)

    async def _ml_translate(self, text: str, source_lang: str, target_lang: str, model_type: str) -> str:
        """Traduction avec le vrai mod√®le ML - tokenizers thread-local pour √©viter 'Already borrowed'"""
        try:
            if model_type not in self.models:
                raise Exception(f"Mod√®le {model_type} non charg√©")
            
            # CORRECTION: Sauvegarder le model_name original pour √©viter les collisions dans la boucle de fallback
            original_model_name = self.model_configs[model_type]['model_name']
            
            # Traduction dans un thread - OPTIMISATION: tokenizer thread-local cach√©
            def translate():
                try:
                    from transformers import pipeline
                    import threading
                    
                    # Mod√®le partag√© (thread-safe en lecture)
                    shared_model = self.models[model_type]
                    
                    # OPTIMISATION: Utiliser le tokenizer thread-local cach√© (√©vite recr√©ation)
                    thread_tokenizer = self._get_thread_local_tokenizer(model_type)
                    if thread_tokenizer is None:
                        raise Exception(f"Impossible d'obtenir le tokenizer pour {model_type}")
                    
                    # Diff√©rencier T5 et NLLB avec pipelines appropri√©s
                    if "t5" in original_model_name.lower():
                        # T5: utiliser text2text-generation avec tokenizer thread-local
                        # OPTIMISATION CPU: R√©duire max_length pour acc√©l√©rer les inf√©rences
                        temp_pipeline = pipeline(
                            "text2text-generation",
                            model=shared_model,
                            tokenizer=thread_tokenizer,  # ‚Üê TOKENIZER THREAD-LOCAL
                            device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                            max_length=128,  # R√©duit de 512 √† 128 pour la vitesse
                            batch_size=4  # Traiter un texte √† la fois sur CPU
                        )
                        
                        # T5: format avec noms complets de langues
                        source_name = self.language_names.get(source_lang, source_lang.capitalize())
                        target_name = self.language_names.get(target_lang, target_lang.capitalize())
                        instruction = f"translate {source_name} to {target_name}: {text}"
                        
                        logger.debug(f"T5 instruction: {instruction}")
                        
                        # OPTIMISATION CPU: R√©duire num_beams pour acc√©l√©rer consid√©rablement
                        # num_beams=1 (greedy) est 4x plus rapide que num_beams=4
                        result = temp_pipeline(
                            instruction,
                            max_new_tokens=128,  # Augment√© de 32 √† 128 pour traductions plus longues
                            num_beams=2,  # R√©duit de 4 √† 2 (greedy search = 2x plus rapide)
                            do_sample=False,
                            early_stopping=True,
                            repetition_penalty=1.1,
                            length_penalty=1.0
                        )
                        
                        # T5 retourne generated_text
                        t5_success = False
                        translated = None  # CORRECTION: Initialiser translated pour √©viter UnboundLocalError
                        if result and len(result) > 0 and 'generated_text' in result[0]:
                            raw_text = result[0]['generated_text']
                            
                            # Nettoyer l'instruction si pr√©sente dans le r√©sultat
                            instruction_prefix = f"translate {source_name} to {target_name}:"
                            if instruction_prefix in raw_text:
                                parts = raw_text.split(instruction_prefix, 1)
                                if len(parts) > 1:
                                    translated = parts[1].strip()
                                else:
                                    translated = raw_text.strip()
                            else:
                                translated = raw_text.strip()
                                
                            # Validation: v√©rifier si T5 a vraiment traduit (pas juste r√©p√©t√© l'instruction)
                            # Rejeter SEULEMENT si:
                            # 1. Vide
                            # 2. Identique √† l'original
                            # 3. Contient l'instruction compl√®te T5 (ex: "translate French to Spanish:")
                            has_instruction = f"translate {source_name} to {target_name}:" in translated.lower()
                            
                            if not translated or translated.lower() == text.lower() or has_instruction:
                                logger.warning(f"T5 traduction invalide: '{translated}', fallback vers NLLB")
                                t5_success = False
                            else:
                                t5_success = True
                                
                        # Si T5 √©choue, fallback automatique vers NLLB
                        if not t5_success:
                            logger.info(f"Fallback : T5 ‚Üí NLLB {source_lang}‚Üí{target_lang}")
                            # Nettoyer le pipeline T5 (tokenizer reste en cache)
                            del temp_pipeline
                            
                            # CORRECTION: Chercher un mod√®le NLLB parmi les mod√®les charg√©s
                            # Les cl√©s sont 'basic', 'medium', 'premium', pas les noms de mod√®les
                            nllb_model_type = None
                            nllb_model_name = None
                            
                            # Chercher medium ou premium (qui sont des mod√®les NLLB)
                            # CORRECTION: Utiliser un nom de variable diff√©rent pour √©viter la collision
                            for fallback_model_type in ['medium', 'premium']:
                                if fallback_model_type in self.models:
                                    config = self.model_configs.get(fallback_model_type, {})
                                    fallback_model_name = config.get('model_name', '')
                                    if 'nllb' in fallback_model_name.lower():
                                        nllb_model_type = fallback_model_type
                                        nllb_model_name = fallback_model_name
                                        break
                            
                            if nllb_model_type is None:
                                logger.warning(f"Mod√®le NLLB non charg√©, impossible de faire le fallback")
                                translated = f"[Translation-Failed] {text}"
                            else:
                                nllb_model = self.models[nllb_model_type]
                                # OPTIMISATION: Utiliser tokenizer thread-local cach√©
                                nllb_tokenizer = self._get_thread_local_tokenizer(nllb_model_type)
                                if nllb_tokenizer is None:
                                    logger.warning(f"Impossible d'obtenir tokenizer NLLB")
                                    translated = f"[Translation-Failed] {text}"
                                    return translated
                                
                                # OPTIMISATION CPU: Param√®tres optimis√©s pour la vitesse
                                nllb_pipeline = pipeline(
                                    "translation",
                                    model=nllb_model,
                                    tokenizer=nllb_tokenizer,
                                    device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                                    max_length=256,  # Augment√© de 64 √† 256 pour traductions plus longues
                                    batch_size=2  # Traiter un texte √† la fois sur CPU
                                )
                                
                                nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                                nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')
                                
                                nllb_result = nllb_pipeline(
                                    text, 
                                    src_lang=nllb_source, 
                                    tgt_lang=nllb_target, 
                                    max_length=256,  # Augment√© de 64 √† 512 pour traductions plus longues
                                    num_beams=2,  # Greedy search = 4x plus rapide
                                    early_stopping=True
                                )
                                
                                if nllb_result and len(nllb_result) > 0 and 'translation_text' in nllb_result[0]:
                                    translated = nllb_result[0]['translation_text']
                                    logger.info(f"‚úÖ Fallback NLLB r√©ussi: '{text}' ‚Üí '{translated}'")
                                else:
                                    translated = f"[NLLB-Fallback-Failed] {text}"
                                
                                # Nettoyer le pipeline (tokenizer reste en cache)
                                del nllb_pipeline
                            
                            return translated
                            
                    else:
                        # NLLB: utiliser translation avec tokenizer thread-local
                        # OPTIMISATION CPU: R√©duire max_length et batch_size
                        temp_pipeline = pipeline(
                            "translation",
                            model=shared_model,
                            tokenizer=thread_tokenizer,  # ‚Üê TOKENIZER THREAD-LOCAL
                            device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                            max_length=256,  # Augment√© de 64 √† 256 pour traductions plus longues
                            batch_size=2  # Traiter un texte √† la fois sur CPU
                        )
                        
                        # NLLB: codes de langue sp√©ciaux
                        nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                        nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')
                        
                        # OPTIMISATION CPU: Param√®tres optimis√©s pour la vitesse
                        result = temp_pipeline(
                            text, 
                            src_lang=nllb_source, 
                            tgt_lang=nllb_target, 
                            max_length=256,  # Augment√© de 64 √† 256 pour traductions plus longues
                            num_beams=2,  # Greedy search = 4x plus rapide
                            early_stopping=True
                        )
                        
                        # NLLB retourne translation_text
                        if result and len(result) > 0 and 'translation_text' in result[0]:
                            translated = result[0]['translation_text']
                        else:
                            translated = f"[NLLB-No-Result] {text}"
                    
                    # Nettoyer tokenizer et pipeline temporaires
                    del thread_tokenizer
                    del temp_pipeline
                    
                    return translated
                    
                except Exception as e:
                    logger.error(f"Erreur pipeline {original_model_name}: {e}")
                    return f"[ML-Pipeline-Error] {text}"
            
            # Ex√©cuter de mani√®re asynchrone
            loop = asyncio.get_event_loop()
            translated = await loop.run_in_executor(self.executor, translate)
            
            return translated
            
        except Exception as e:
            logger.error(f"‚ùå Erreur mod√®le ML {model_type}: {e}")
            return f"[ML-Error] {text}"
    
    def _detect_language(self, text: str) -> str:
        """D√©tection de langue simple"""
        text_lower = text.lower()
        
        # Mots caract√©ristiques par langue
        if any(word in text_lower for word in ['bonjour', 'comment', 'vous', 'merci', 'salut']):
            return 'fr'
        elif any(word in text_lower for word in ['hello', 'how', 'you', 'thank', 'hi']):
            return 'en'
        elif any(word in text_lower for word in ['hola', 'como', 'estas', 'gracias']):
            return 'es'
        elif any(word in text_lower for word in ['guten', 'wie', 'geht', 'danke', 'hallo']):
            return 'de'
        else:
            return 'en'  # D√©faut
    
    async def _fallback_translate(self, text: str, source_lang: str, target_lang: str, 
                                 model_type: str, source_channel: str) -> Dict[str, Any]:
        """Traduction de fallback si ML non disponible"""
        logger.warning(f"Utilisation du fallback pour {model_type} [{source_channel}]")
        
        # Dictionnaire simple comme fallback
        translations = {
            ('fr', 'en'): {
                'bonjour': 'hello', 'comment': 'how', 'vous': 'you', 'allez': 'are',
                '√™tes': 'are', 'tout': 'all', 'le': 'the', 'monde': 'world'
            },
            ('en', 'fr'): {
                'hello': 'bonjour', 'how': 'comment', 'you': 'vous', 'are': '√™tes',
                'all': 'tout', 'the': 'le', 'world': 'monde'
            },
            ('es', 'fr'): {
                'hola': 'bonjour', 'como': 'comment', 'estas': 'allez-vous'
            },
            ('en', 'de'): {
                'hello': 'hallo', 'how': 'wie', 'are': 'sind', 'you': 'sie'
            }
        }
        
        # Traduction simple mot par mot
        lang_pair = (source_lang, target_lang)
        if lang_pair in translations:
            words = text.lower().split()
            translated_words = []
            for word in words:
                translated_word = translations[lang_pair].get(word, word)
                translated_words.append(translated_word)
            translated_text = ' '.join(translated_words)
        else:
            translated_text = f"[FALLBACK-{source_lang}‚Üí{target_lang}] {text}"
        
        self._update_stats(0.001, source_channel)
        
        return {
            'translated_text': translated_text,
            'detected_language': source_lang,
            'confidence': 0.3,  # Faible confiance pour fallback
            'model_used': f"{model_type}_fallback",
            'from_cache': False,
            'processing_time': 0.001,
            'source_channel': source_channel
        }
    
    def _update_stats(self, processing_time: float, source_channel: str):
        """Met √† jour les statistiques globales"""
        self.stats['translations_count'] += 1
        
        # Mettre √† jour les stats par canal (canaux connus seulement)
        if source_channel in ['zmq', 'rest', 'websocket']:
            self.stats[f'{source_channel}_translations'] += 1
        
        self.request_times.append(processing_time)
        
        if len(self.request_times) > 200:
            self.request_times = self.request_times[-200:]
        
        if self.request_times:
            self.stats['avg_processing_time'] = sum(self.request_times) / len(self.request_times)
    
    async def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques globales du service"""
        return {
            'service_type': 'unified_ml',
            'is_singleton': True,
            'translations_count': self.stats['translations_count'],
            'zmq_translations': self.stats['zmq_translations'],
            'rest_translations': self.stats['rest_translations'], 
            'websocket_translations': self.stats['websocket_translations'],
            'avg_processing_time': self.stats['avg_processing_time'],
            'models_loaded': {
                model_type: {
                    'name': self.model_configs[model_type]['model_name'],
                    'description': self.model_configs[model_type]['description'],
                    'local_path': str(self.model_configs[model_type]['local_path']),
                    'is_local': self.model_configs[model_type]['local_path'].exists()
                } for model_type in self.models.keys()
            },
            'ml_available': ML_AVAILABLE,
            'is_initialized': self.is_initialized,
            'startup_time': self.stats['startup_time'],
            'supported_languages': list(self.lang_codes.keys()),
            'models_path': str(self.models_path),
            'device': self.device
        }
    
    async def get_health(self) -> Dict[str, Any]:
        """Health check du service unifi√©"""
        return {
            'status': 'healthy' if self.is_initialized else 'initializing',
            'models_count': len(self.models),
            'pipelines_count': len(self.pipelines),
            'ml_available': ML_AVAILABLE,
            'translations_served': self.stats['translations_count']
        }

# Instance globale du service (Singleton)
def get_unified_ml_service(max_workers: int = 4) -> TranslationMLService:
    """Retourne l'instance unique du service ML"""
    return TranslationMLService(get_settings(), max_workers=max_workers)
