"""
Service de traduction ML unifiÃ© - Architecture centralisÃ©e
Un seul service ML qui charge les modÃ¨les au dÃ©marrage et sert tous les canaux
"""

import os
import logging
import time
import asyncio
from typing import Dict, Optional, List, Any, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import threading
from pathlib import Path

# Import des settings
from config.settings import get_settings

# Import des modÃ¨les ML optimisÃ©s
try:
    import torch
    
    # SOLUTION: DÃ©sactiver les tensors meta avant d'importer les autres modules
    torch._C._disable_meta = True  # DÃ©sactiver les tensors meta au niveau PyTorch
    
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
    ML_AVAILABLE = True
    
    # Suppression des warnings de retry Xet
    import warnings
    warnings.filterwarnings("ignore", message=".*Retry attempt.*")
    warnings.filterwarnings("ignore", message=".*reqwest.*")
    warnings.filterwarnings("ignore", message=".*xethub.*")
    warnings.filterwarnings("ignore", message=".*IncompleteMessage.*")
    warnings.filterwarnings("ignore", message=".*SendRequest.*")
    
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸ Dependencies ML non disponibles")

logger = logging.getLogger(__name__)

@dataclass
class TranslationResult:
    """RÃ©sultat d'une traduction unifiÃ©"""
    translated_text: str
    detected_language: str
    confidence: float
    model_used: str
    from_cache: bool
    processing_time: float
    source_channel: str  # 'zmq', 'rest', 'websocket'

class TranslationMLService:
    """
    Service de traduction ML unifiÃ© - Singleton
    Charge les modÃ¨les une seule fois au dÃ©marrage et sert tous les canaux
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        """Singleton pattern pour garantir une seule instance"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, settings, model_type: str = "all", max_workers: int = 4, quantization_level: str = "float16"):
        if self._initialized:
            return
            
        # Charger les settings
        self.settings = settings
        
        self.model_type = model_type
        # OPTIMISATION CPU: Limiter les workers pour Ã©viter le context switching
        # Sur CPU, 2-4 workers suffisent largement
        import os
        cpu_workers = min(max_workers, int(os.getenv('ML_MAX_WORKERS', '4')))
        self.max_workers = cpu_workers
        self.quantization_level = quantization_level
        self.executor = ThreadPoolExecutor(max_workers=cpu_workers)
        
        # ModÃ¨les ML chargÃ©s (partagÃ©s entre tous les canaux)
        self.models = {}
        self.tokenizers = {}
        self.pipelines = {}
        
        # Cache thread-local de tokenizers pour Ã©viter "Already borrowed"
        self._thread_local_tokenizers = {}
        self._tokenizer_lock = threading.Lock()
        
        # Configuration des modÃ¨les depuis les settings et .env
        self.models_path = Path(self.settings.models_path)
        self.device = os.getenv('DEVICE', 'cpu')
        
        self.model_configs = {
            'basic': {
                'model_name': self.settings.basic_model,
                'local_path': self.models_path / self.settings.basic_model,
                'description': f'{self.settings.basic_model} - ModÃ¨le rapide',
                'device': self.device,
                'priority': 1  # ChargÃ© en premier
            },
            'medium': {
                'model_name': self.settings.medium_model,
                'local_path': self.models_path / self.settings.medium_model,
                'description': f'{self.settings.medium_model} - ModÃ¨le Ã©quilibrÃ©',
                'device': self.device,
                'priority': 2
            },
            'premium': {
                'model_name': self.settings.premium_model,
                'local_path': self.models_path / self.settings.premium_model,
                'description': f'{self.settings.premium_model} - ModÃ¨le haute qualitÃ©',
                'device': self.device,
                'priority': 3
            }
        }
        
        # Mapping des codes de langues NLLB
        self.lang_codes = {
            'fr': 'fra_Latn',
            'en': 'eng_Latn', 
            'es': 'spa_Latn',
            'de': 'deu_Latn',
            'pt': 'por_Latn',
            'zh': 'zho_Hans',
            'ja': 'jpn_Jpan',
            'ar': 'arb_Arab'
        }
        
        # Mapping des noms de langues pour T5 (noms complets)
        self.language_names = {
            'fr': 'French',
            'en': 'English', 
            'es': 'Spanish',
            'de': 'German',
            'pt': 'Portuguese',
            'zh': 'Chinese',
            'ja': 'Japanese',
            'ar': 'Arabic',
            'it': 'Italian',
            'ru': 'Russian',
            'ko': 'Korean',
            'nl': 'Dutch'
        }
        
        # Stats globales (partagÃ©es entre tous les canaux)
        self.stats = {
            'translations_count': 0,
            'zmq_translations': 0,
            'rest_translations': 0,
            'websocket_translations': 0,
            'avg_processing_time': 0.0,
            'models_loaded': False,
            'startup_time': None
        }
        self.request_times = []
        
        # Ã‰tat d'initialisation
        self.is_initialized = False
        self.is_loading = False
        self._startup_lock = asyncio.Lock()
        
        self._initialized = True
        self._configure_environment()
        logger.info(f"ðŸ¤– Service ML UnifiÃ© crÃ©Ã© (Singleton) avec {max_workers} workers")
    
    def _configure_environment(self):
        """Configure les variables d'environnement basÃ©es sur les settings"""
        import os
        
        # OPTIMISATION XET: Configuration pour rÃ©duire les warnings du nouveau systÃ¨me
        os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
        os.environ['HF_HUB_DISABLE_IMPLICIT_TOKEN'] = '1'
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        
        # OPTIMISATION RÃ‰SEAU: Configuration pour amÃ©liorer la connectivitÃ© Docker
        os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'
        os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = str(self.settings.huggingface_timeout)
        os.environ['HF_HUB_DOWNLOAD_RETRY_DELAY'] = '5'
        os.environ['HF_HUB_DOWNLOAD_MAX_RETRIES'] = str(self.settings.model_download_max_retries)
        
        # SOLUTION: DÃ©sactiver les tensors meta pour Ã©viter l'erreur Tensor.item()
        os.environ['PYTORCH_DISABLE_META'] = '1'
        os.environ['PYTORCH_FORCE_CUDA'] = '0'  # Forcer CPU si pas de GPU
        os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = '1'
        
        # Configuration pour Ã©viter les problÃ¨mes de proxy/corporate network
        # VÃ©rifier si le fichier de certificats existe, sinon utiliser le systÃ¨me par dÃ©faut
        if os.path.exists('/etc/ssl/certs/ca-certificates.crt'):
            os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
            os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
        elif os.path.exists('/etc/ssl/certs/ca-bundle.crt'):
            os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
            os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
        else:
            # Utiliser le systÃ¨me par dÃ©faut
            logger.info("âš ï¸ Fichier de certificats SSL non trouvÃ©, utilisation du systÃ¨me par dÃ©faut")
        
        # Option pour dÃ©sactiver temporairement la vÃ©rification SSL si nÃ©cessaire
        if os.getenv('HF_HUB_DISABLE_SSL_VERIFICATION', '0') == '1':
            os.environ['REQUESTS_CA_BUNDLE'] = ''
            os.environ['CURL_CA_BUNDLE'] = ''
            logger.info("âš ï¸ VÃ©rification SSL dÃ©sactivÃ©e pour Hugging Face (HF_HUB_DISABLE_SSL_VERIFICATION=1)")
    
    async def initialize(self) -> bool:
        """Initialise les modÃ¨les ML une seule fois au dÃ©marrage"""
        async with self._startup_lock:
            if self.is_initialized:
                logger.info("âœ… Service ML dÃ©jÃ  initialisÃ©")
                return True
                
            if self.is_loading:
                logger.info("â³ Initialisation ML en cours...")
                # Attendre que l'initialisation se termine
                while self.is_loading and not self.is_initialized:
                    await asyncio.sleep(0.5)
                return self.is_initialized
            
            self.is_loading = True
            startup_start = time.time()
            
            if not ML_AVAILABLE:
                logger.error("âŒ Transformers non disponible. Service ML dÃ©sactivÃ©.")
                self.is_loading = False
                return False
            
            try:
                logger.info("ðŸš€ Initialisation du Service ML UnifiÃ©...")
                logger.info("ðŸ“š Chargement des modÃ¨les NLLB...")
                
                # Charger les modÃ¨les par ordre de prioritÃ©
                models_to_load = sorted(
                    self.model_configs.items(), 
                    key=lambda x: x[1]['priority']
                )
                
                for model_type, config in models_to_load:
                    try:
                        await self._load_model(model_type)
                    except Exception as e:
                        logger.error(f"âŒ Erreur chargement {model_type}: {e}")
                        # Continuer avec les autres modÃ¨les
                
                # VÃ©rifier qu'au moins un modÃ¨le est chargÃ©
                if not self.models:
                    logger.error("âŒ Aucun modÃ¨le ML chargÃ©")
                    self.is_loading = False
                    return False
                
                startup_time = time.time() - startup_start
                self.stats['startup_time'] = startup_time
                self.stats['models_loaded'] = True
                self.is_initialized = True
                self.is_loading = False
                
                logger.info(f"âœ… Service ML UnifiÃ© initialisÃ© en {startup_time:.2f}s")
                logger.info(f"ðŸ“Š ModÃ¨les chargÃ©s: {list(self.models.keys())}")
                logger.info(f"ðŸŽ¯ PrÃªt Ã  servir tous les canaux: ZMQ, REST, WebSocket")
                
                return True
                
            except Exception as e:
                logger.error(f"âŒ Erreur critique initialisation ML: {e}")
                self.is_loading = False
                return False
    
    def _get_thread_local_tokenizer(self, model_type: str) -> Optional[AutoTokenizer]:
        """Obtient ou crÃ©e un tokenizer pour le thread actuel (Ã©vite 'Already borrowed')"""
        import threading
        thread_id = threading.current_thread().ident
        cache_key = f"{model_type}_{thread_id}"
        
        # VÃ©rifier le cache thread-local
        if cache_key in self._thread_local_tokenizers:
            return self._thread_local_tokenizers[cache_key]
        
        # CrÃ©er un nouveau tokenizer pour ce thread
        with self._tokenizer_lock:
            # Double-check aprÃ¨s acquisition du lock
            if cache_key in self._thread_local_tokenizers:
                return self._thread_local_tokenizers[cache_key]
            
            try:
                model_name = self.model_configs[model_type]['model_name']
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir=str(self.models_path),
                    use_fast=True
                )
                self._thread_local_tokenizers[cache_key] = tokenizer
                logger.debug(f"âœ… Tokenizer thread-local crÃ©Ã©: {cache_key}")
                return tokenizer
            except Exception as e:
                logger.error(f"âŒ Erreur crÃ©ation tokenizer thread-local: {e}")
                return None
    
    async def _load_model(self, model_type: str):
        """Charge un modÃ¨le spÃ©cifique depuis local ou HuggingFace"""
        if model_type in self.models:
            return  # DÃ©jÃ  chargÃ©
        
        config = self.model_configs[model_type]
        model_name = config['model_name']
        local_path = config['local_path']
        device = config['device']
        
        logger.info(f"ðŸ“¥ Chargement {model_type}: {model_name}")
        
        # Charger dans un thread pour Ã©viter de bloquer
        def load_model():
            try:
                # Tokenizer
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name, 
                    cache_dir=str(self.models_path),
                    use_fast=True,  # Tokenizer rapide
                    model_max_length=512  # Limiter la taille
                )
                
                # ModÃ¨le avec quantification
                # OPTIMISATION CPU: Utiliser float32 au lieu de float16 sur CPU pour Ã©viter les erreurs
                # et amÃ©liorer la compatibilitÃ©. Sur CPU, float16 n'apporte pas d'accÃ©lÃ©ration.
                dtype = torch.float32 if device == "cpu" else (
                    getattr(torch, self.quantization_level) if hasattr(torch, self.quantization_level) else torch.float32
                )
                
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name,
                    cache_dir=str(self.models_path), 
                    torch_dtype=dtype,
                    low_cpu_mem_usage=True,  # Optimisation mÃ©moire
                    device_map="auto" if device == "cuda" else None
                )
                
                # OPTIMISATION CPU: Mettre le modÃ¨le en mode eval pour dÃ©sactiver dropout
                model.eval()
                
                # CORRECTION: Pas de pipeline partagÃ© pour Ã©viter "Already borrowed"
                # On crÃ©e les pipelines Ã  la demande dans _ml_translate
                
                return tokenizer, model
                
            except Exception as e:
                logger.error(f"âŒ Erreur chargement {model_type}: {e}")
                return None, None
        
        # Charger de maniÃ¨re asynchrone
        loop = asyncio.get_event_loop()
        tokenizer, model = await loop.run_in_executor(self.executor, load_model)
        
        if model and tokenizer:
            self.tokenizers[model_type] = tokenizer
            self.models[model_type] = model
            logger.info(f"âœ… ModÃ¨le {model_type} chargÃ©: {model_name}")
            if local_path.exists():
                logger.info(f"ðŸ“ ModÃ¨le disponible en local: {local_path}")
        else:
            raise Exception(f"Ã‰chec chargement {model_type}")
    
    async def translate(self, text: str, source_language: str = "auto", 
                       target_language: str = "en", model_type: str = "basic",
                       source_channel: str = "unknown") -> Dict[str, Any]:
        """
        Interface unique de traduction pour tous les canaux
        source_channel: 'zmq', 'rest', 'websocket'
        """
        start_time = time.time()
        
        try:
            # Validation
            if not text.strip():
                raise ValueError("Text cannot be empty")
            
            # VÃ©rifier que le service est initialisÃ©
            if not self.is_initialized:
                logger.warning("Service ML non initialisÃ©, utilisation du fallback")
                return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)
            
            # Fallback si modÃ¨le spÃ©cifique pas disponible  
            if model_type not in self.models:
                # Utiliser le premier modÃ¨le disponible
                available_models = list(self.models.keys())
                if available_models:
                    model_type = available_models[0]
                    logger.info(f"ModÃ¨le demandÃ© non disponible, utilisation de: {model_type}")
                else:
                    return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)
            
            # DÃ©tecter la langue source si nÃ©cessaire
            detected_lang = source_language if source_language != "auto" else self._detect_language(text)
            
            # Traduire avec le vrai modÃ¨le ML
            translated_text = await self._ml_translate(text, detected_lang, target_language, model_type)
            
            processing_time = time.time() - start_time
            self._update_stats(processing_time, source_channel)
            
            result = {
                'translated_text': translated_text,
                'detected_language': detected_lang,
                'confidence': 0.95,  # Confiance Ã©levÃ©e pour les vrais modÃ¨les
                'model_used': f"{model_type}_ml",
                'from_cache': False,
                'processing_time': processing_time,
                'source_channel': source_channel
            }
            
            logger.info(f"âœ… [ML-{source_channel.upper()}] '{text[:20]}...' â†’ '{translated_text[:20]}...' ({processing_time:.3f}s)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Erreur traduction ML [{source_channel}]: {e}")
            # Fallback en cas d'erreur
            return await self._fallback_translate(text, source_language, target_language, model_type, source_channel)
    
    async def _ml_translate(self, text: str, source_lang: str, target_lang: str, model_type: str) -> str:
        """Traduction avec le vrai modÃ¨le ML - tokenizers thread-local pour Ã©viter 'Already borrowed'"""
        try:
            if model_type not in self.models:
                raise Exception(f"ModÃ¨le {model_type} non chargÃ©")
            
            # CORRECTION: Sauvegarder le model_name original pour Ã©viter les collisions dans la boucle de fallback
            original_model_name = self.model_configs[model_type]['model_name']
            
            # Traduction dans un thread - OPTIMISATION: tokenizer thread-local cachÃ©
            def translate():
                try:
                    from transformers import pipeline
                    import threading
                    
                    # ModÃ¨le partagÃ© (thread-safe en lecture)
                    shared_model = self.models[model_type]
                    
                    # OPTIMISATION: Utiliser le tokenizer thread-local cachÃ© (Ã©vite recrÃ©ation)
                    thread_tokenizer = self._get_thread_local_tokenizer(model_type)
                    if thread_tokenizer is None:
                        raise Exception(f"Impossible d'obtenir le tokenizer pour {model_type}")
                    
                    # DiffÃ©rencier T5 et NLLB avec pipelines appropriÃ©s
                    if "t5" in original_model_name.lower():
                        # T5: utiliser text2text-generation avec tokenizer thread-local
                        # OPTIMISATION CPU: RÃ©duire max_length pour accÃ©lÃ©rer les infÃ©rences
                        temp_pipeline = pipeline(
                            "text2text-generation",
                            model=shared_model,
                            tokenizer=thread_tokenizer,  # â† TOKENIZER THREAD-LOCAL
                            device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                            max_length=64,  # RÃ©duit de 128 Ã  64 pour la vitesse
                            batch_size=1  # Traiter un texte Ã  la fois sur CPU
                        )
                        
                        # T5: format avec noms complets de langues
                        source_name = self.language_names.get(source_lang, source_lang.capitalize())
                        target_name = self.language_names.get(target_lang, target_lang.capitalize())
                        instruction = f"translate {source_name} to {target_name}: {text}"
                        
                        logger.debug(f"T5 instruction: {instruction}")
                        
                        # OPTIMISATION CPU: RÃ©duire num_beams pour accÃ©lÃ©rer considÃ©rablement
                        # num_beams=1 (greedy) est 4x plus rapide que num_beams=4
                        result = temp_pipeline(
                            instruction,
                            max_new_tokens=32,  # RÃ©duit de 64 Ã  32
                            num_beams=1,  # RÃ©duit de 4 Ã  1 (greedy search = 4x plus rapide)
                            do_sample=False,
                            early_stopping=True,
                            repetition_penalty=1.1,
                            length_penalty=1.0
                        )
                        
                        # T5 retourne generated_text
                        t5_success = False
                        translated = None  # CORRECTION: Initialiser translated pour Ã©viter UnboundLocalError
                        if result and len(result) > 0 and 'generated_text' in result[0]:
                            raw_text = result[0]['generated_text']
                            
                            # Nettoyer l'instruction si prÃ©sente dans le rÃ©sultat
                            instruction_prefix = f"translate {source_name} to {target_name}:"
                            if instruction_prefix in raw_text:
                                parts = raw_text.split(instruction_prefix, 1)
                                if len(parts) > 1:
                                    translated = parts[1].strip()
                                else:
                                    translated = raw_text.strip()
                            else:
                                translated = raw_text.strip()
                                
                            # Validation: vÃ©rifier si T5 a vraiment traduit (pas juste rÃ©pÃ©tÃ© l'instruction)
                            # Rejeter SEULEMENT si:
                            # 1. Vide
                            # 2. Identique Ã  l'original
                            # 3. Contient l'instruction complÃ¨te T5 (ex: "translate French to Spanish:")
                            has_instruction = f"translate {source_name} to {target_name}:" in translated.lower()
                            
                            if not translated or translated.lower() == text.lower() or has_instruction:
                                logger.warning(f"T5 traduction invalide: '{translated}', fallback vers NLLB")
                                t5_success = False
                            else:
                                t5_success = True
                                
                        # Si T5 Ã©choue, fallback automatique vers NLLB
                        if not t5_success:
                            logger.info(f"ðŸ”„ Fallback automatique: T5 â†’ NLLB pour {source_lang}â†’{target_lang}")
                            # Nettoyer le pipeline T5 (tokenizer reste en cache)
                            del temp_pipeline
                            
                            # CORRECTION: Chercher un modÃ¨le NLLB parmi les modÃ¨les chargÃ©s
                            # Les clÃ©s sont 'basic', 'medium', 'premium', pas les noms de modÃ¨les
                            nllb_model_type = None
                            nllb_model_name = None
                            
                            # Chercher medium ou premium (qui sont des modÃ¨les NLLB)
                            # CORRECTION: Utiliser un nom de variable diffÃ©rent pour Ã©viter la collision
                            for fallback_model_type in ['medium', 'premium']:
                                if fallback_model_type in self.models:
                                    config = self.model_configs.get(fallback_model_type, {})
                                    fallback_model_name = config.get('model_name', '')
                                    if 'nllb' in fallback_model_name.lower():
                                        nllb_model_type = fallback_model_type
                                        nllb_model_name = fallback_model_name
                                        break
                            
                            if nllb_model_type is None:
                                logger.warning(f"ModÃ¨le NLLB non chargÃ©, impossible de faire le fallback")
                                translated = f"[Translation-Failed] {text}"
                            else:
                                nllb_model = self.models[nllb_model_type]
                                # OPTIMISATION: Utiliser tokenizer thread-local cachÃ©
                                nllb_tokenizer = self._get_thread_local_tokenizer(nllb_model_type)
                                if nllb_tokenizer is None:
                                    logger.warning(f"Impossible d'obtenir tokenizer NLLB")
                                    translated = f"[Translation-Failed] {text}"
                                    return translated
                                
                                # OPTIMISATION CPU: ParamÃ¨tres optimisÃ©s pour la vitesse
                                nllb_pipeline = pipeline(
                                    "translation",
                                    model=nllb_model,
                                    tokenizer=nllb_tokenizer,
                                    device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                                    max_length=64,  # RÃ©duit de 128 Ã  64
                                    batch_size=1  # Traiter un texte Ã  la fois sur CPU
                                )
                                
                                nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                                nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')
                                
                                nllb_result = nllb_pipeline(
                                    text, 
                                    src_lang=nllb_source, 
                                    tgt_lang=nllb_target, 
                                    max_length=64,  # RÃ©duit de 128 Ã  64
                                    num_beams=1,  # Greedy search = 4x plus rapide
                                    early_stopping=True
                                )
                                
                                if nllb_result and len(nllb_result) > 0 and 'translation_text' in nllb_result[0]:
                                    translated = nllb_result[0]['translation_text']
                                    logger.info(f"âœ… Fallback NLLB rÃ©ussi: '{text}' â†’ '{translated}'")
                                else:
                                    translated = f"[NLLB-Fallback-Failed] {text}"
                                
                                # Nettoyer le pipeline (tokenizer reste en cache)
                                del nllb_pipeline
                            
                            return translated
                            
                    else:
                        # NLLB: utiliser translation avec tokenizer thread-local
                        # OPTIMISATION CPU: RÃ©duire max_length et batch_size
                        temp_pipeline = pipeline(
                            "translation",
                            model=shared_model,
                            tokenizer=thread_tokenizer,  # â† TOKENIZER THREAD-LOCAL
                            device=0 if self.device == 'cuda' and torch.cuda.is_available() else -1,
                            max_length=64,  # RÃ©duit de 128 Ã  64
                            batch_size=1  # Traiter un texte Ã  la fois sur CPU
                        )
                        
                        # NLLB: codes de langue spÃ©ciaux
                        nllb_source = self.lang_codes.get(source_lang, 'eng_Latn')
                        nllb_target = self.lang_codes.get(target_lang, 'fra_Latn')
                        
                        # OPTIMISATION CPU: ParamÃ¨tres optimisÃ©s pour la vitesse
                        result = temp_pipeline(
                            text, 
                            src_lang=nllb_source, 
                            tgt_lang=nllb_target, 
                            max_length=64,  # RÃ©duit de 128 Ã  64
                            num_beams=1,  # Greedy search = 4x plus rapide
                            early_stopping=True
                        )
                        
                        # NLLB retourne translation_text
                        if result and len(result) > 0 and 'translation_text' in result[0]:
                            translated = result[0]['translation_text']
                        else:
                            translated = f"[NLLB-No-Result] {text}"
                    
                    # Nettoyer tokenizer et pipeline temporaires
                    del thread_tokenizer
                    del temp_pipeline
                    
                    return translated
                    
                except Exception as e:
                    logger.error(f"Erreur pipeline {original_model_name}: {e}")
                    return f"[ML-Pipeline-Error] {text}"
            
            # ExÃ©cuter de maniÃ¨re asynchrone
            loop = asyncio.get_event_loop()
            translated = await loop.run_in_executor(self.executor, translate)
            
            return translated
            
        except Exception as e:
            logger.error(f"âŒ Erreur modÃ¨le ML {model_type}: {e}")
            return f"[ML-Error] {text}"
    
    def _detect_language(self, text: str) -> str:
        """DÃ©tection de langue simple"""
        text_lower = text.lower()
        
        # Mots caractÃ©ristiques par langue
        if any(word in text_lower for word in ['bonjour', 'comment', 'vous', 'merci', 'salut']):
            return 'fr'
        elif any(word in text_lower for word in ['hello', 'how', 'you', 'thank', 'hi']):
            return 'en'
        elif any(word in text_lower for word in ['hola', 'como', 'estas', 'gracias']):
            return 'es'
        elif any(word in text_lower for word in ['guten', 'wie', 'geht', 'danke', 'hallo']):
            return 'de'
        else:
            return 'en'  # DÃ©faut
    
    async def _fallback_translate(self, text: str, source_lang: str, target_lang: str, 
                                 model_type: str, source_channel: str) -> Dict[str, Any]:
        """Traduction de fallback si ML non disponible"""
        logger.warning(f"Utilisation du fallback pour {model_type} [{source_channel}]")
        
        # Dictionnaire simple comme fallback
        translations = {
            ('fr', 'en'): {
                'bonjour': 'hello', 'comment': 'how', 'vous': 'you', 'allez': 'are',
                'Ãªtes': 'are', 'tout': 'all', 'le': 'the', 'monde': 'world'
            },
            ('en', 'fr'): {
                'hello': 'bonjour', 'how': 'comment', 'you': 'vous', 'are': 'Ãªtes',
                'all': 'tout', 'the': 'le', 'world': 'monde'
            },
            ('es', 'fr'): {
                'hola': 'bonjour', 'como': 'comment', 'estas': 'allez-vous'
            },
            ('en', 'de'): {
                'hello': 'hallo', 'how': 'wie', 'are': 'sind', 'you': 'sie'
            }
        }
        
        # Traduction simple mot par mot
        lang_pair = (source_lang, target_lang)
        if lang_pair in translations:
            words = text.lower().split()
            translated_words = []
            for word in words:
                translated_word = translations[lang_pair].get(word, word)
                translated_words.append(translated_word)
            translated_text = ' '.join(translated_words)
        else:
            translated_text = f"[FALLBACK-{source_lang}â†’{target_lang}] {text}"
        
        self._update_stats(0.001, source_channel)
        
        return {
            'translated_text': translated_text,
            'detected_language': source_lang,
            'confidence': 0.3,  # Faible confiance pour fallback
            'model_used': f"{model_type}_fallback",
            'from_cache': False,
            'processing_time': 0.001,
            'source_channel': source_channel
        }
    
    def _update_stats(self, processing_time: float, source_channel: str):
        """Met Ã  jour les statistiques globales"""
        self.stats['translations_count'] += 1
        
        # Mettre Ã  jour les stats par canal (canaux connus seulement)
        if source_channel in ['zmq', 'rest', 'websocket']:
            self.stats[f'{source_channel}_translations'] += 1
        
        self.request_times.append(processing_time)
        
        if len(self.request_times) > 200:
            self.request_times = self.request_times[-200:]
        
        if self.request_times:
            self.stats['avg_processing_time'] = sum(self.request_times) / len(self.request_times)
    
    async def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques globales du service"""
        return {
            'service_type': 'unified_ml',
            'is_singleton': True,
            'translations_count': self.stats['translations_count'],
            'zmq_translations': self.stats['zmq_translations'],
            'rest_translations': self.stats['rest_translations'], 
            'websocket_translations': self.stats['websocket_translations'],
            'avg_processing_time': self.stats['avg_processing_time'],
            'models_loaded': {
                model_type: {
                    'name': self.model_configs[model_type]['model_name'],
                    'description': self.model_configs[model_type]['description'],
                    'local_path': str(self.model_configs[model_type]['local_path']),
                    'is_local': self.model_configs[model_type]['local_path'].exists()
                } for model_type in self.models.keys()
            },
            'ml_available': ML_AVAILABLE,
            'is_initialized': self.is_initialized,
            'startup_time': self.stats['startup_time'],
            'supported_languages': list(self.lang_codes.keys()),
            'models_path': str(self.models_path),
            'device': self.device
        }
    
    async def get_health(self) -> Dict[str, Any]:
        """Health check du service unifiÃ©"""
        return {
            'status': 'healthy' if self.is_initialized else 'initializing',
            'models_count': len(self.models),
            'pipelines_count': len(self.pipelines),
            'ml_available': ML_AVAILABLE,
            'translations_served': self.stats['translations_count']
        }

# Instance globale du service (Singleton)
def get_unified_ml_service(max_workers: int = 4) -> TranslationMLService:
    """Retourne l'instance unique du service ML"""
    return TranslationMLService(get_settings(), max_workers=max_workers)
