"""
Service de traduction ML quantifi√© - Version optimis√©e
Optimisations: R√©duction des boucles, cache intelligent, chargement concurrent
"""

import os
import logging
import time
import asyncio
from typing import Dict, Optional, Any, Set, Tuple
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict
from functools import lru_cache
import threading

# Import des settings
from config.settings import get_settings



# Import des mod√®les ML optimis√©s
try:
    import torch
    
    # SOLUTION: D√©sactiver les tensors meta avant d'importer les autres modules
    torch._C._disable_meta = True  # D√©sactiver les tensors meta au niveau PyTorch
    
    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
    ML_AVAILABLE = True
    
    # OPTIMISATION XET: Configuration pour r√©duire les warnings du nouveau syst√®me
    import os
    os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
    os.environ['HF_HUB_DISABLE_IMPLICIT_TOKEN'] = '1'
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'
    
    # OPTIMISATION R√âSEAU: Configuration pour am√©liorer la connectivit√© Docker
    os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'
    os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '600'  # 10 minutes
    os.environ['HF_HUB_DOWNLOAD_RETRY_DELAY'] = '5'
    os.environ['HF_HUB_DOWNLOAD_MAX_RETRIES'] = '5'
    
    # SOLUTION: D√©sactiver les tensors meta pour √©viter l'erreur Tensor.item()
    os.environ['PYTORCH_DISABLE_META'] = '1'
    os.environ['PYTORCH_FORCE_CUDA'] = '0'  # Forcer CPU si pas de GPU
    os.environ['PYTORCH_NO_CUDA_MEMORY_CACHING'] = '1'
    
    # Configuration pour √©viter les probl√®mes de proxy/corporate network
    # V√©rifier si le fichier de certificats existe, sinon utiliser le syst√®me par d√©faut
    if os.path.exists('/etc/ssl/certs/ca-certificates.crt'):
        os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
        os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
    elif os.path.exists('/etc/ssl/certs/ca-bundle.crt'):
        os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
        os.environ['CURL_CA_BUNDLE'] = '/etc/ssl/certs/ca-bundle.crt'
    else:
        # Utiliser le syst√®me par d√©faut
        print("‚ö†Ô∏è Fichier de certificats SSL non trouv√©, utilisation du syst√®me par d√©faut")
    
    # Option pour d√©sactiver temporairement la v√©rification SSL si n√©cessaire
    if os.getenv('HF_HUB_DISABLE_SSL_VERIFICATION', '0') == '1':
        os.environ['REQUESTS_CA_BUNDLE'] = ''
        os.environ['CURL_CA_BUNDLE'] = ''
        print("‚ö†Ô∏è V√©rification SSL d√©sactiv√©e pour Hugging Face (HF_HUB_DISABLE_SSL_VERIFICATION=1)")
    
    # Suppression des warnings de retry Xet
    import warnings
    warnings.filterwarnings("ignore", message=".*Retry attempt.*")
    warnings.filterwarnings("ignore", message=".*reqwest.*")
    warnings.filterwarnings("ignore", message=".*xethub.*")
    warnings.filterwarnings("ignore", message=".*IncompleteMessage.*")
    warnings.filterwarnings("ignore", message=".*SendRequest.*")
    
except ImportError:
    ML_AVAILABLE = False
    print("‚ö†Ô∏è Dependencies ML non disponibles")

logger = logging.getLogger(__name__)

class QuantizedMLService:
    """
    Service de traduction ML quantifi√© optimis√©
    - R√©duction drastique des boucles
    - Chargement concurrent des mod√®les
    - Cache intelligent pour les configurations
    - Optimisations m√©moire
    """
    
    def __init__(self, model_type: str = "basic", quantization_level: str = "float16", max_workers: int = 4):
        self.settings = get_settings()
        self.model_type = model_type
        self.quantization_level = quantization_level
        self.max_workers = max_workers
        

        
        # Cache des configurations avec lazy loading
        self._model_configs = None
        self._shared_models_analysis = None
        
        # Mod√®les et √©tats
        self.models = {}
        self.tokenizers = {}
        self.shared_models = {}
        self.model_to_shared = {}
        
        # Thread pool optimis√©
        self.executor = ThreadPoolExecutor(
            max_workers=max_workers,
            thread_name_prefix="MLQuantized"
        )
        
        # Stats optimis√©es
        self.stats = {
            'translations_count': 0,
            'avg_processing_time': 0.0,
            'memory_usage_mb': 0.0,
            'quantization_level': quantization_level,
            'models_loaded': False,
            'shared_models_count': 0,
            'memory_saved_mb': 0.0,
            'cache_hits': 0,
            'concurrent_loads': 0
        }
        
        # Caches pour √©viter les recalculs
        self._lang_codes_cache = None
        self._language_names_cache = None
        
        logger.info(f"ü§ñ Service ML Quantifi√© Optimis√© cr√©√©: {model_type} avec {quantization_level}")
    
    @property
    def model_configs(self) -> Dict:
        """Configuration des mod√®les avec cache lazy"""
        if self._model_configs is None:
            self._model_configs = {
                'basic': {
                    'model_name': self.settings.basic_model,
                    'description': f'{self.settings.basic_model} quantifi√©',
                    'max_length': 128
                },
                'medium': {
                    'model_name': self.settings.medium_model,
                    'description': f'{self.settings.medium_model} quantifi√©',
                    'max_length': 256
                },
                'premium': {
                    'model_name': self.settings.premium_model,
                    'description': f'{self.settings.premium_model} quantifi√©',
                    'max_length': 512
                }
            }
        return self._model_configs
    
    @property
    def lang_codes(self) -> Dict:
        """Codes de langues avec cache"""
        if self._lang_codes_cache is None:
            self._lang_codes_cache = {
                'fr': 'fra_Latn', 'en': 'eng_Latn', 'es': 'spa_Latn', 'de': 'deu_Latn',
                'pt': 'por_Latn', 'zh': 'zho_Hans', 'ja': 'jpn_Jpan', 'ar': 'arb_Arab'
            }
        return self._lang_codes_cache
    
    @property
    def language_names(self) -> Dict:
        """Noms de langues avec cache"""
        if self._language_names_cache is None:
            self._language_names_cache = {
                'fr': 'French', 'en': 'English', 'es': 'Spanish', 'de': 'German',
                'pt': 'Portuguese', 'zh': 'Chinese', 'ja': 'Japanese', 'ar': 'Arabic'
            }
        return self._language_names_cache
    
    @lru_cache(maxsize=1)
    def _get_shared_models_analysis(self) -> Tuple[Dict[str, list], Set[str]]:
        """Analyse des mod√®les partag√©s avec cache LRU"""
        model_name_to_types = defaultdict(list)
        
        # Une seule boucle pour construire le mapping
        for model_type, config in self.model_configs.items():
            model_name_to_types[config['model_name']].append(model_type)
        
        # Identifier les mod√®les partag√©s en une passe
        shared_models_info = {
            model_name: types for model_name, types in model_name_to_types.items() 
            if len(types) > 1
        }
        
        unique_model_names = set(model_name_to_types.keys())
        
        return shared_models_info, unique_model_names
    
    async def initialize(self) -> bool:
        """Initialisation optimis√©e avec chargement concurrent"""
        if not ML_AVAILABLE:
            logger.error("‚ùå Dependencies ML non disponibles")
            return False
        
        try:
            logger.info(f"üöÄ Initialisation optimis√©e: {self.model_type} ({self.quantization_level})")
            
            # Pas de nettoyage automatique pour √©viter les probl√®mes
            
            if self.model_type == "all":
                # Chargement concurrent de tous les mod√®les
                await self._load_all_models_concurrently()
            else:
                # Chargement optimis√© d'un mod√®le avec fallback
                await self._load_model_with_optimized_fallback()
            
            self.stats['models_loaded'] = True
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation optimis√©e: {e}")
            return False
    
    async def _load_all_models_concurrently(self):
        """Charge tous les mod√®les de fa√ßon concurrente - Version simplifi√©e"""
        shared_models_info, _ = self._get_shared_models_analysis()
        
        # Cr√©er les t√¢ches de chargement
        load_tasks = []
        processed_models = set()
        
        for model_type, config in self.model_configs.items():
            model_name = config['model_name']
            
            if model_name in shared_models_info and model_name not in processed_models:
                # Charger comme mod√®le partag√©
                task = asyncio.create_task(
                    self._load_shared_model_async(model_name, shared_models_info[model_name])
                )
                load_tasks.append(task)
                processed_models.add(model_name)
            elif model_name not in shared_models_info:
                # Charger comme mod√®le unique
                task = asyncio.create_task(
                    self._load_unique_model_async(model_name, model_type)
                )
                load_tasks.append(task)
        
        # Attendre tous les chargements en parall√®le
        if load_tasks:
            self.stats['concurrent_loads'] = len(load_tasks)
            results = await asyncio.gather(*load_tasks, return_exceptions=True)
            
            # V√©rifier les r√©sultats
            failed_loads = [r for r in results if isinstance(r, Exception)]
            if failed_loads:
                logger.warning(f"‚ö†Ô∏è {len(failed_loads)} chargements √©chou√©s sur {len(results)}")
            
            logger.info(f"‚úÖ Chargement concurrent termin√©: {len(results) - len(failed_loads)}/{len(results)} succ√®s")
    

    
    async def _load_model_with_optimized_fallback(self):
        """Tentative de chargement avec fallback - Version simplifi√©e"""
        fallback_order = ['basic', 'medium', 'premium']
        
        # Calculer les mod√®les candidats en une seule passe
        try:
            user_index = fallback_order.index(self.model_type)
            candidate_models = fallback_order[user_index::-1]  # Du demand√© vers le plus l√©ger
        except ValueError:
            candidate_models = fallback_order
        
        # Essayer les candidats s√©quentiellement
        last_error = None
        for model_type in candidate_models:
            if model_type not in self.model_configs:
                continue
                
            try:
                await self._load_model_with_sharing_optimized(model_type)
                if model_type != self.model_type:
                    logger.info(f"üîÑ Fallback vers {model_type} (demand√©: {self.model_type})")
                    self.model_type = model_type
                return
            except Exception as e:
                last_error = e
                logger.warning(f"‚ö†Ô∏è √âchec {model_type}: {e}")
        
        # Si aucun mod√®le ne peut √™tre charg√©, lever une exception claire
        error_msg = f"√âchec du chargement de tous les mod√®les de traduction. Derni√®re erreur: {last_error}"
        logger.error(f"‚ùå {error_msg}")
        raise Exception(error_msg)
    
    async def _load_model_with_sharing_optimized(self, model_type: str):
        """Chargement optimis√© avec gestion du partage"""
        config = self.model_configs[model_type]
        model_name = config['model_name']
        
        # V√©rifier le cache partag√©
        if model_name in self.shared_models:
            self._link_to_shared_model(model_name, model_type)
            return
        
        shared_models_info, _ = self._get_shared_models_analysis()
        
        if model_name in shared_models_info:
            # Charger comme mod√®le partag√©
            await self._load_shared_model_async(model_name, shared_models_info[model_name])
        else:
            # Charger comme mod√®le unique
            await self._load_unique_model_async(model_name, model_type)
    
    def _link_to_shared_model(self, model_name: str, model_type: str):
        """Lie un type de mod√®le √† un mod√®le partag√© existant"""
        self.shared_models[model_name]['users'].add(model_type)
        self.model_to_shared[model_type] = model_name
        self.models[model_type] = self.shared_models[model_name]['model']
        self.tokenizers[model_type] = self.shared_models[model_name]['tokenizer']
        self.stats['cache_hits'] += 1
        logger.info(f"üîó {model_type} li√© au mod√®le partag√© {model_name}")
    
    async def _load_shared_model_async(self, model_name: str, model_types: list):
        """Charge un mod√®le partag√© de fa√ßon asynchrone"""
        logger.info(f"üì• Chargement mod√®le partag√©: {model_name} pour {model_types}")
        
        try:
            model, tokenizer = await self._load_model_and_tokenizer_optimized(model_name)
            
            # Cr√©er l'entr√©e partag√©e
            self.shared_models[model_name] = {
                'model': model,
                'tokenizer': tokenizer,
                'users': set(model_types),
                'loaded_at': time.time()
            }
            
            # Lier tous les types en une passe
            for model_type in model_types:
                self.model_to_shared[model_type] = model_name
                self.models[model_type] = model
                self.tokenizers[model_type] = tokenizer
            
            logger.info(f"‚úÖ Mod√®le partag√© charg√©: {model_name}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur mod√®le partag√© {model_name}: {e}")
            raise
    
    async def _load_unique_model_async(self, model_name: str, model_type: str):
        """Charge un mod√®le unique de fa√ßon asynchrone"""
        logger.info(f"üì• Chargement mod√®le unique: {model_type} ({model_name})")
        
        try:
            model, tokenizer = await self._load_model_and_tokenizer_optimized(model_name)
            self.models[model_type] = model
            self.tokenizers[model_type] = tokenizer
            logger.info(f"‚úÖ Mod√®le unique charg√©: {model_type}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur mod√®le unique {model_type}: {e}")
            raise
    
    async def _load_model_and_tokenizer_optimized(self, model_name: str) -> Tuple:
        """Chargement optimis√© avec retry exponentiel - Version simplifi√©e et robuste"""
        async def load_with_timeout(loader_func, timeout: int, description: str):
            try:
                loop = asyncio.get_event_loop()
                return await asyncio.wait_for(
                    loop.run_in_executor(self.executor, loader_func),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                raise Exception(f"Timeout {description} apr√®s {timeout}s")
        
        # Chargement concurrent tokenizer + mod√®le avec timeouts
        tokenizer_task = load_with_timeout(
            lambda: self._load_tokenizer_sync(model_name),
            self.settings.tokenizer_load_timeout,
            "tokenizer"
        )
        
        model_task = load_with_timeout(
            lambda: self._load_model_sync(model_name),
            self.settings.model_load_timeout,
            "mod√®le"
        )
        
        # Chargement parall√®le
        tokenizer, model = await asyncio.gather(tokenizer_task, model_task)
        return model, tokenizer
    
    def _load_tokenizer_sync(self, model_name: str):
        """Chargement synchrone du tokenizer - Version robuste avec retry et fallback"""
        import warnings
        import os
        import time
        import requests
        from pathlib import Path
        from huggingface_hub import HfApi
        
        # Configuration du cache local - utiliser self.settings
        cache_dir = Path(self.settings.models_path)
        cache_dir.mkdir(exist_ok=True)
        
        # Configuration des retries
        max_retries = int(os.getenv('HF_HUB_DOWNLOAD_MAX_RETRIES', '5'))
        retry_delay = int(os.getenv('HF_HUB_DOWNLOAD_RETRY_DELAY', '5'))
        
        last_error = None
        
        for attempt in range(max_retries):
            try:
                logger.info(f"üì• Tentative {attempt + 1}/{max_retries} de chargement du tokenizer {model_name}")
                
                # Test de connectivit√© Hugging Face
                try:
                    api = HfApi()
                    api.model_info(model_name, timeout=30)
                    logger.info(f"‚úÖ Connectivit√© Hugging Face OK pour {model_name}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Probl√®me de connectivit√© HF pour {model_name}: {e}")
                    # Continuer quand m√™me, peut-√™tre que le mod√®le est en cache local
                
                # Suppression temporaire des warnings pendant le chargement
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    
                    # Configuration optimis√©e pour Docker
                    tokenizer = AutoTokenizer.from_pretrained(
                        model_name,
                        local_files_only=False,
                        trust_remote_code=True,
                        cache_dir=str(cache_dir),
                        use_auth_token=None,  # Pas de token pour les mod√®les publics
                        resume_download=True,  # Reprendre les t√©l√©chargements interrompus
                        force_download=False,  # Ne pas forcer le re-t√©l√©chargement
                        proxies=None  # Pas de proxy par d√©faut
                    )
                
                logger.info(f"‚úÖ Tokenizer {model_name} charg√© avec succ√®s")
                return tokenizer
                
            except Exception as e:
                last_error = e
                logger.warning(f"‚ö†Ô∏è Tentative {attempt + 1} √©chou√©e pour {model_name}: {e}")
                
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)  # Backoff exponentiel
                    logger.info(f"‚è≥ Attente de {wait_time}s avant la prochaine tentative...")
                    time.sleep(wait_time)
        
        # Si toutes les tentatives ont √©chou√©, essayer avec des param√®tres de fallback
        logger.warning(f"üîÑ Tentative de fallback pour le tokenizer {model_name}")
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                return AutoTokenizer.from_pretrained(
                    model_name,
                    local_files_only=True,  # Essayer seulement le cache local
                    trust_remote_code=True,
                    cache_dir=str(cache_dir)
                )
        except Exception as fallback_error:
            logger.error(f"‚ùå √âchec du fallback pour {model_name}: {fallback_error}")
            raise last_error or fallback_error
    
    def _load_model_sync(self, model_name: str):
        """Chargement synchrone du mod√®le avec quantification - Version robuste avec retry et fallback"""
        import warnings
        import os
        import time
        from pathlib import Path
        from huggingface_hub import HfApi
        
        # Configuration du cache local - utiliser self.settings
        cache_dir = Path(self.settings.models_path)
        cache_dir.mkdir(exist_ok=True)
        
        # Configuration des retries
        max_retries = int(os.getenv('HF_HUB_DOWNLOAD_MAX_RETRIES', '5'))
        retry_delay = int(os.getenv('HF_HUB_DOWNLOAD_RETRY_DELAY', '5'))
        
        # Configuration de base optimis√©e pour Docker avec d√©sactivation compl√®te des tensors meta
        base_config = {
            'local_files_only': False,
            'trust_remote_code': True,
            'cache_dir': str(cache_dir),
            'use_auth_token': None,  # Pas de token pour les mod√®les publics
            'resume_download': True,  # Reprendre les t√©l√©chargements interrompus
            'force_download': False,  # Ne pas forcer le re-t√©l√©chargement
            'proxies': None,  # Pas de proxy par d√©faut
            'low_cpu_mem_usage': False,  # D√©sactiver l'optimisation m√©moire qui peut causer des tensors meta
            'torch_dtype': torch.float32,  # Forcer float32 pour √©viter les meta tensors
            'attn_implementation': 'eager',  # Forcer l'impl√©mentation eager pour √©viter les meta tensors
            'device_map': None  # Pas de device mapping automatique
        }
        
        # Configuration forc√©e pour √©viter les meta tensors
        # Note: torch_dtype et attn_implementation sont d√©j√† d√©finis dans base_config
        
        last_error = None
        
        for attempt in range(max_retries):
            try:
                logger.info(f"üì• Tentative {attempt + 1}/{max_retries} de chargement du mod√®le {model_name}")
                
                # Test de connectivit√© Hugging Face
                try:
                    api = HfApi()
                    api.model_info(model_name, timeout=30)
                    logger.info(f"‚úÖ Connectivit√© Hugging Face OK pour {model_name}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Probl√®me de connectivit√© HF pour {model_name}: {e}")
                    # Continuer quand m√™me, peut-√™tre que le mod√®le est en cache local
                
                # Suppression temporaire des warnings pendant le chargement
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    
                                    # Chargement du mod√®le avec d√©sactivation des tensors meta
                model = AutoModelForSeq2SeqLM.from_pretrained(
                    model_name, 
                    **base_config
                )
                
                # Initialisation simple sans dummy input pour √©viter les erreurs
                model.eval()  # Mode √©valuation
                logger.info(f"‚úÖ Mod√®le {model_name} charg√© avec succ√®s")
                
                # Quantification int8 si demand√©e
                if self.quantization_level == "int8":
                    try:
                        model = torch.quantization.quantize_dynamic(
                            model, {torch.nn.Linear}, dtype=torch.qint8
                        )
                        logger.info(f"‚úÖ Mod√®le {model_name} quantifi√© en int8")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Quantification int8 √©chou√©e pour {model_name}, mod√®le en float32: {e}")
                
                logger.info(f"‚úÖ Mod√®le {model_name} charg√© avec succ√®s")
                return model
                
            except Exception as e:
                last_error = e
                logger.warning(f"‚ö†Ô∏è Tentative {attempt + 1} √©chou√©e pour {model_name}: {e}")
                
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)  # Backoff exponentiel
                    logger.info(f"‚è≥ Attente de {wait_time}s avant la prochaine tentative...")
                    time.sleep(wait_time)
        
        # Si toutes les tentatives ont √©chou√©, essayer avec des param√®tres de fallback
        logger.warning(f"üîÑ Tentative de fallback pour le mod√®le {model_name}")
        try:
            fallback_config = {
                'local_files_only': True,  # Essayer seulement le cache local
                'trust_remote_code': True,
                'cache_dir': str(cache_dir),
                'torch_dtype': torch.float32,  # Fallback vers float32
                'low_cpu_mem_usage': False,   # D√©sactiver l'optimisation m√©moire
                'device_map': None,           # Pas de device mapping automatique
                'attn_implementation': 'eager'  # Forcer eager
            }
            
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                model = AutoModelForSeq2SeqLM.from_pretrained(model_name, **fallback_config)
                
                # Initialisation simple pour le fallback aussi
                model.eval()
                logger.info(f"‚úÖ Mod√®le {model_name} charg√© en fallback (cache local)")
                return model
                
        except Exception as fallback_error:
            logger.error(f"‚ùå √âchec du fallback pour {model_name}: {fallback_error}")
            raise last_error or fallback_error
    

    
    async def translate(self, text: str, source_language: str, target_language: str, 
                       model_type: str = None, source_channel: str = "quantized") -> Dict[str, Any]:
        """Traduction optimis√©e avec fallback intelligent"""
        if model_type is None:
            model_type = self.model_type
        
        start_time = time.time()
        
        # OPTIMISATION: √âviter la traduction si source = target
        if source_language == target_language:
            logger.info(f"üîÑ [TRANSLATOR] Langues identiques ({source_language} ‚Üí {target_language}), pas de traduction n√©cessaire")
            return {
                'translated_text': text,
                'detected_language': source_language,
                'confidence': 1.0,
                'model_used': f"none",
                'from_cache': False,
                'processing_time': 0.0,
                'source_channel': source_channel
            }
        
        try:
            # Fallback optimis√© sans boucles multiples
            best_model = self._find_best_available_model(model_type)
            
            # Traduction
            translated_text = await self._ml_translate_optimized(
                text, source_language, target_language, best_model
            )
            
            # Mise √† jour des stats optimis√©e
            processing_time = time.time() - start_time
            self._update_stats(processing_time)
            
            return {
                'translated_text': translated_text,
                'detected_language': source_language,
                'confidence': 0.95,
                'model_used': f"{best_model}_{self.quantization_level}",
                'from_cache': False,
                'processing_time': processing_time,
                'source_channel': source_channel
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur traduction: {e}")
            processing_time = time.time() - start_time
            
            # Retourner un message d'√©chec clair
            error_message = f"[√âCHEC TRADUCTION] {text}"
            if "Aucun mod√®le de traduction disponible" in str(e):
                error_message = f"[MOD√àLES NON DISPONIBLES] {text}"
            elif "√âchec du chargement" in str(e):
                error_message = f"[MOD√àLES NON CHARG√âS] {text}"
            
            return {
                'translated_text': error_message,
                'detected_language': source_language,
                'confidence': 0.0,
                'model_used': f"{model_type}_{self.quantization_level}_error",
                'from_cache': False,
                'processing_time': processing_time,
                'source_channel': source_channel,
                'error': str(e)
            }
    
    def _find_best_available_model(self, requested_model: str) -> str:
        """Trouve le meilleur mod√®le disponible sans boucles multiples"""
        if requested_model in self.models:
            return requested_model
        
        # Ordre de qualit√© d√©croissante
        quality_order = ['premium', 'medium', 'basic']
        
        try:
            requested_index = quality_order.index(requested_model)
            # Chercher depuis le mod√®le demand√© vers les moins performants
            candidates = quality_order[requested_index:]
        except ValueError:
            candidates = quality_order
        
        # Retourner le premier mod√®le disponible
        for model_type in candidates:
            if model_type in self.models:
                return model_type
        
        # Si aucun mod√®le n'est disponible, lever une exception claire
        available_models = list(self.models.keys())
        error_msg = f"Aucun mod√®le de traduction disponible. Mod√®les demand√©s: {requested_model}, Mod√®les charg√©s: {available_models}"
        logger.error(f"‚ùå {error_msg}")
        raise Exception(error_msg)
    
    def _update_stats(self, processing_time: float):
        """Mise √† jour optimis√©e des statistiques"""
        count = self.stats['translations_count']
        self.stats['translations_count'] = count + 1
        
        # Moyenne mobile optimis√©e
        if count > 0:
            current_avg = self.stats['avg_processing_time']
            self.stats['avg_processing_time'] = (current_avg * count + processing_time) / (count + 1)
        else:
            self.stats['avg_processing_time'] = processing_time
    
    async def _ml_translate_optimized(self, text: str, source_language: str, 
                                    target_language: str, model_type: str) -> str:
        """Traduction ML optimis√©e avec tokenization thread-local"""
        def translate_sync():
            try:
                # Nettoyage m√©moire optimis√©
                self._cleanup_memory()
                
                model = self.models[model_type]
                model_name = self.model_configs[model_type]['model_name']
                
                # SOLUTION: Cr√©er un tokenizer unique pour ce thread
                import threading
                thread_id = threading.current_thread().ident
                cache_key = f"{model_type}_{thread_id}"
                
                # Mod√®le partag√© (thread-safe en lecture) mais tokenizer local
                model_path = self.model_configs[model_type]['model_name']
                
                # OPTIMISATION: Cr√©er un tokenizer frais avec param√®tres optimis√©s
                thread_tokenizer = AutoTokenizer.from_pretrained(
                    model_path,
                    cache_dir=str(self.settings.models_path),
                    local_files_only=True,  # Utiliser le mod√®le local
                    use_fast=True,  # Tokenizer rapide
                    model_max_length=512,  # Limiter la taille
                    torch_dtype=torch.float32  # Forcer float32
                )
                
                # D√©tection automatique du type de mod√®le
                is_t5_model = "t5" in model_name.lower()
                
                # OPTIMISATION: Diff√©rencier T5 et NLLB avec pipelines appropri√©s
                if is_t5_model:
                    # T5: utiliser text2text-generation avec tokenizer thread-local
                    temp_pipeline = pipeline(
                        "text2text-generation",
                        model=model,
                        tokenizer=thread_tokenizer,  # ‚Üê TOKENIZER THREAD-LOCAL
                        device=-1,  # Forcer CPU pour √©viter les probl√®mes de tensors meta
                        max_length=128,
                        torch_dtype=torch.float32  # Forcer float32
                    )
                    
                    # T5: format avec noms complets de langues
                    source_name = self.language_names.get(source_language, source_language.capitalize())
                    target_name = self.language_names.get(target_language, target_language.capitalize())
                    instruction = f"translate {source_name} to {target_name}: {text}"
                    
                    # OPTIMISATION: Traduction avec param√®tres optimis√©s
                    result = temp_pipeline(
                        instruction,
                        max_new_tokens=64,
                        num_beams=2,  # R√©duire pour √©conomiser la m√©moire
                        do_sample=False,
                        early_stopping=True,
                        repetition_penalty=1.1,
                        length_penalty=1.0,
                        pad_token_id=thread_tokenizer.eos_token_id
                    )
                    
                    # T5 retourne generated_text
                    t5_success = False
                    if result and len(result) > 0 and 'generated_text' in result[0]:
                        raw_text = result[0]['generated_text']
                        
                        # Nettoyer l'instruction si pr√©sente dans le r√©sultat
                        instruction_prefix = f"translate {source_name} to {target_name}:"
                        if instruction_prefix in raw_text:
                            parts = raw_text.split(instruction_prefix, 1)
                            if len(parts) > 1:
                                translated = parts[1].strip()
                            else:
                                translated = raw_text.strip()
                        else:
                            translated = raw_text.strip()
                            
                        # Validation: v√©rifier si T5 a vraiment traduit (pas juste r√©p√©t√© l'instruction)
                        # Rejeter SEULEMENT si:
                        # 1. Vide
                        # 2. Identique √† l'original
                        # 3. Contient l'instruction compl√®te T5 (ex: "translate French to Spanish:")
                        has_instruction = f"translate {source_name} to {target_name}:" in translated.lower()
                        
                        if not translated or translated.lower() == text.lower() or has_instruction:
                            logger.warning(f"T5 traduction invalide: '{translated}', fallback vers NLLB")
                            t5_success = False
                        else:
                            t5_success = True
                            
                    # Si T5 √©choue, fallback automatique vers NLLB
                    if not t5_success:
                        logger.info(f"üîÑ Fallback automatique: T5 ‚Üí NLLB pour {source_language}‚Üí{target_language}")
                        # Nettoyer le pipeline T5
                        del temp_pipeline
                        del thread_tokenizer
                        
                        # CORRECTION: Chercher un mod√®le NLLB parmi les mod√®les charg√©s
                        # Les cl√©s sont 'basic', 'medium', 'premium', pas les noms de mod√®les
                        nllb_model_type = None
                        nllb_model_name = None
                        
                        # Chercher medium ou premium (qui sont des mod√®les NLLB)
                        for model_type_key in ['medium', 'premium']:
                            if model_type_key in self.models:
                                config = self.model_configs.get(model_type_key, {})
                                model_name = config.get('model_name', '')
                                if 'nllb' in model_name.lower():
                                    nllb_model_type = model_type_key
                                    nllb_model_name = model_name
                                    break
                        
                        if nllb_model_type is None:
                            logger.warning(f"Mod√®le NLLB non charg√©, impossible de faire le fallback")
                            translated = f"[Translation-Failed] {text}"
                        else:
                            try:
                                # Utiliser le mod√®le NLLB d√©j√† charg√©
                                nllb_model = self.models[nllb_model_type]
                                nllb_tokenizer = AutoTokenizer.from_pretrained(
                                    nllb_model_name,
                                    cache_dir=str(self.settings.models_path),
                                    local_files_only=True,
                                    use_fast=True
                                )
                                
                                nllb_pipeline = pipeline(
                                    "translation",
                                    model=nllb_model,
                                    tokenizer=nllb_tokenizer,
                                    device=-1,
                                    max_length=128,
                                    torch_dtype=torch.float32
                                )
                                
                                nllb_source = self.lang_codes.get(source_language, 'eng_Latn')
                                nllb_target = self.lang_codes.get(target_language, 'fra_Latn')
                                
                                nllb_result = nllb_pipeline(
                                    text, 
                                    src_lang=nllb_source, 
                                    tgt_lang=nllb_target, 
                                    max_length=128,
                                    num_beams=2,
                                    early_stopping=True
                                )
                                
                                if nllb_result and len(nllb_result) > 0 and 'translation_text' in nllb_result[0]:
                                    translated = nllb_result[0]['translation_text']
                                    logger.info(f"‚úÖ Fallback NLLB r√©ussi: '{text}' ‚Üí '{translated}'")
                                else:
                                    translated = f"[NLLB-Fallback-Failed] {text}"
                                
                                # Nettoyer (pas besoin de supprimer nllb_model car c'est self.models[nllb_model_type])
                                del nllb_tokenizer
                                del nllb_pipeline
                                
                            except Exception as e:
                                logger.error(f"‚ùå Erreur fallback NLLB: {e}")
                                translated = f"[NLLB-Fallback-Error] {text}"
                        
                        # CORRECTION: thread_tokenizer d√©j√† supprim√©, ne pas le supprimer √† nouveau
                        return translated
                        
                else:
                    # NLLB: utiliser translation avec tokenizer thread-local
                    temp_pipeline = pipeline(
                        "translation",
                        model=model,
                        tokenizer=thread_tokenizer,  # ‚Üê TOKENIZER THREAD-LOCAL
                        device=0 if torch.cuda.is_available() else -1,
                        max_length=128,
                        torch_dtype=torch.float32  # Type de donn√©es standard
                    )
                    
                    # NLLB: codes de langue sp√©ciaux
                    nllb_source = self.lang_codes.get(source_language, 'eng_Latn')
                    nllb_target = self.lang_codes.get(target_language, 'fra_Latn')
                    
                    # OPTIMISATION: Traduction avec param√®tres optimis√©s
                    result = temp_pipeline(
                        text, 
                        src_lang=nllb_source, 
                        tgt_lang=nllb_target, 
                        max_length=128,
                        num_beams=2,  # R√©duire pour √©conomiser la m√©moire
                        early_stopping=True
                    )
                    
                    # NLLB retourne translation_text
                    if result and len(result) > 0 and 'translation_text' in result[0]:
                        translated = result[0]['translation_text']
                    else:
                        translated = f"[NLLB-No-Result] {text}"
                
                # OPTIMISATION: Nettoyer tokenizer et pipeline temporaires
                del thread_tokenizer
                del temp_pipeline
                
                # Nettoyer la m√©moire apr√®s traduction
                import gc
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                return translated
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur pipeline: {e}")
                return f"[ML-Error] {text}"
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, translate_sync)
    
    def _cleanup_memory(self):
        """Nettoyage m√©moire optimis√©"""
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def get_stats(self) -> Dict[str, Any]:
        """Stats optimis√©es avec calculs en cache"""
        shared_models_count = len(self.shared_models)
        unique_models = len(set(self.model_to_shared.values())) + len([
            t for t in self.models.keys() if t not in self.model_to_shared
        ])
        
        return {
            **self.stats,
            'model_type': self.model_type,
            'shared_models_count': shared_models_count,
            'unique_models_loaded': unique_models,
            'models_saved': len(self.model_configs) - unique_models,
            'optimization_ratio': f"{unique_models}/{len(self.model_configs)}"
        }
    
    def get_available_models(self) -> list:
        """Retourne les mod√®les disponibles"""
        return list(self.models.keys())
    
    async def cleanup(self):
        """Nettoyage optimis√©"""
        logger.info("üßπ Nettoyage optimis√© des ressources")
        
        # Nettoyage group√© des ressources
        resources_to_clean = [
            (self.shared_models, "mod√®les partag√©s"),
            (self.models, "mod√®les"),
            (self.tokenizers, "tokenizers")
        ]
        
        for resource_dict, resource_type in resources_to_clean:
            if resource_dict:
                logger.info(f"üßπ Nettoyage {len(resource_dict)} {resource_type}")
                resource_dict.clear()
        
        # Nettoyage des mappings
        self.model_to_shared.clear()
        
        # Nettoyage m√©moire et thread pool
        self._cleanup_memory()
        self.executor.shutdown(wait=True)
        
        logger.info("‚úÖ Nettoyage optimis√© termin√©")
    
    async def close(self):
        """Ferme le service et lib√®re les ressources"""
        try:
            logger.info("üõë Fermeture du service ML quantifi√©...")
            
            # Fermer le thread pool
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=True)
            
            # Lib√©rer les mod√®les de la m√©moire
            if hasattr(self, 'models'):
                for model_name, model in self.models.items():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                self.models.clear()
            
            if hasattr(self, 'tokenizers'):
                self.tokenizers.clear()
            
            if hasattr(self, 'shared_models'):
                for model_name, model in self.shared_models.items():
                    if hasattr(model, 'cpu'):
                        model.cpu()
                    del model
                self.shared_models.clear()
            
            logger.info("‚úÖ Service ML quantifi√© ferm√©")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la fermeture: {e}")
    
