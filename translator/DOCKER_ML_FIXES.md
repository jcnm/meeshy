# üîß Solutions pour les probl√®mes de traduction ML dans Docker

## üìã **Probl√®me identifi√©**

Le service de traduction Meeshy dans Docker se bloque apr√®s "Avant appel ML service" et ne parvient pas √† traiter les requ√™tes de traduction.

### **Sympt√¥mes observ√©s :**
- ‚úÖ R√©ception correcte des commandes ZMQ
- ‚úÖ Cr√©ation des t√¢ches de traduction
- ‚ùå Blocage apr√®s "Avant appel ML service"
- ‚ùå Mod√®les ML ne se chargent pas correctement
- ‚ùå Timeouts et erreurs de m√©moire

## üõ†Ô∏è **Solutions impl√©ment√©es**

### **1. Optimisations du service ML unifi√©**

#### **Gestion de m√©moire am√©lior√©e**
```python
# Nettoyage de la m√©moire avant/apr√®s chargement
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# Param√®tres optimis√©s pour Docker
torch_dtype=torch.float32,  # √âconomiser la m√©moire
low_cpu_mem_usage=True,     # Optimisation m√©moire
```

#### **Timeouts et gestion d'erreurs**
```python
# Timeout pour le chargement des mod√®les (5 minutes)
await asyncio.wait_for(
    loop.run_in_executor(self.executor, load_model),
    timeout=300
)

# Timeout pour la traduction (1 minute)
await asyncio.wait_for(
    loop.run_in_executor(self.executor, translate),
    timeout=60
)
```

#### **Tokenizers thread-local**
```python
# Cr√©er un tokenizer unique pour chaque thread
thread_tokenizer = AutoTokenizer.from_pretrained(
    str(model_path),
    cache_dir=str(self.models_path),
    local_files_only=True,
    use_fast=True,  # Tokenizer rapide
    model_max_length=512  # Limiter la taille
)
```

### **2. Configuration Docker optimis√©e**

#### **Limites de ressources ajust√©es**
```yaml
deploy:
  resources:
    limits:
      memory: 8G      # R√©duit de 12G √† 8G
      cpus: '4.0'     # R√©duit de 6.0 √† 4.0
    reservations:
      memory: 4G      # R√©duit de 6G √† 4G
      cpus: '2.0'     # R√©duit de 4.0 √† 2.0
```

#### **Param√®tres ML optimis√©s**
```yaml
environment:
  ML_BATCH_SIZE: 4                    # R√©duit de 16 √† 4
  CONCURRENT_TRANSLATIONS: 5          # R√©duit de 10 √† 5
  TRANSLATION_WORKERS: 4              # R√©duit de 10 √† 4
  NORMAL_WORKERS: 2                   # R√©duit de 3 √† 2
  ANY_WORKERS: 1                      # R√©duit de 2 √† 1
  TRANSLATION_TIMEOUT: 60             # Augment√© de 30 √† 60s
```

#### **Variables d'environnement PyTorch**
```yaml
environment:
  PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:128"
  OMP_NUM_THREADS: 4
  MKL_NUM_THREADS: 4
  NUMEXPR_NUM_THREADS: 4
  TOKENIZERS_PARALLELISM: false
```

### **3. Dockerfile optimis√©**

#### **Pr√©-t√©l√©chargement des mod√®les**
```dockerfile
# Pr√©-t√©l√©chargement du mod√®le de base
RUN python3 -c "import os; from transformers import AutoTokenizer, AutoModelForSeq2SeqLM; import torch; print('üì• T√©l√©chargement t5-small...'); tokenizer = AutoTokenizer.from_pretrained('t5-small', cache_dir='/app/models'); model = AutoModelForSeq2SeqLM.from_pretrained('t5-small', cache_dir='/app/models'); print('‚úÖ t5-small t√©l√©charg√© avec succ√®s'); del tokenizer, model; torch.cuda.empty_cache() if torch.cuda.is_available() else None"
```

#### **D√©pendances syst√®me optimis√©es**
```dockerfile
RUN apt-get install -y --no-install-recommends \
    libopenblas-dev \
    liblapack-dev \
    libatlas-base-dev \
    gfortran \
    pkg-config
```

#### **PyTorch CPU optimis√©**
```dockerfile
RUN pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

## üöÄ **Utilisation des solutions**

### **1. Diagnostic automatique**
```bash
# Ex√©cuter le diagnostic complet
./translator/docker-diagnostic.sh
```

### **2. Red√©marrage optimis√©**
```bash
# Red√©marrage avec optimisations
./translator/restart-translator.sh
```

### **3. Test de traduction**
```bash
# Test simple
curl -X POST "http://localhost:8000/translate" \
  -H "Content-Type: application/json" \
  -d '{"text":"Hello world","source_language":"en","target_language":"fr","model_type":"basic"}'
```

## üìä **Am√©liorations de performance**

### **Avant les optimisations :**
- ‚ùå Blocage apr√®s "Avant appel ML service"
- ‚ùå Timeouts de 30 secondes
- ‚ùå Utilisation m√©moire excessive
- ‚ùå Pas de gestion d'erreurs robuste

### **Apr√®s les optimisations :**
- ‚úÖ Chargement des mod√®les avec timeout
- ‚úÖ Gestion m√©moire optimis√©e
- ‚úÖ Timeouts configurables (60s)
- ‚úÖ Gestion d'erreurs robuste
- ‚úÖ Tokenizers thread-local
- ‚úÖ Pr√©-t√©l√©chargement des mod√®les

## üîç **Monitoring et d√©pannage**

### **Logs √† surveiller :**
```bash
# Surveillance en temps r√©el
docker logs -f translator

# Logs sp√©cifiques ML
docker logs translator | grep -E "(ML|MODEL|TRANSLATION)"
```

### **Indicateurs de succ√®s :**
- ‚úÖ "Service ML unifi√© initialis√© avec succ√®s"
- ‚úÖ "Mod√®les charg√©s: ['basic', 'medium']"
- ‚úÖ "‚úÖ [ML-ZMQ] traduction r√©ussie"

### **Indicateurs de probl√®me :**
- ‚ùå "Timeout lors du chargement du mod√®le"
- ‚ùå "Erreur pipeline"
- ‚ùå "Service ML non initialis√©"

## üéØ **Recommandations**

### **1. Ressources syst√®me**
- **M√©moire :** Minimum 8GB, recommand√© 16GB
- **CPU :** Minimum 4 cores, recommand√© 8 cores
- **Stockage :** Au moins 10GB pour les mod√®les

### **2. Configuration r√©seau**
- **Ports :** 8000 (API), 5555-5558 (ZMQ), 50051 (gRPC)
- **Latence :** < 100ms pour les communications internes

### **3. Mod√®les recommand√©s**
- **D√©veloppement :** `t5-small` (rapide, 60MB)
- **Production :** `nllb-200-distilled-600M` (√©quilibr√©, 1.2GB)
- **Haute qualit√© :** `nllb-200-distilled-1.3B` (pr√©cision, 2.6GB)

## üîÑ **Mise √† jour et maintenance**

### **Red√©marrage p√©riodique**
```bash
# Red√©marrage hebdomadaire recommand√©
0 2 * * 0 /path/to/restart-translator.sh
```

### **Nettoyage des caches**
```bash
# Nettoyage mensuel
docker system prune -f
docker volume prune -f
```

### **Mise √† jour des mod√®les**
```bash
# Forcer le re-t√©l√©chargement
docker-compose down
docker volume rm meeshy_translator_models
docker-compose up -d translator
```

## üìû **Support**

En cas de probl√®me persistant :
1. Ex√©cuter le diagnostic : `./docker-diagnostic.sh`
2. V√©rifier les logs : `docker logs -f translator`
3. Red√©marrer avec optimisations : `./restart-translator.sh`
4. Consulter cette documentation pour les solutions sp√©cifiques
